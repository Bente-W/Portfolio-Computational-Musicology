---
title: "It's a kind of magic"
author: "Bente Westermann"
date: '2023'
output: 
  flexdashboard::flex_dashboard:
    theme:
      bg: "#fffffa"
      fg: "#1c1b1b" 
      primary: "#8fbc8f"
      navbar-bg: "#a9dea9"
      base_font: 
        google: Source Sans Pro
      heading_font:
        google: Sen
---

```{r, setup, echo=FALSE}
# loading packages
library(tidyverse)
library(spotifyr)
library(ggpattern)
library(ggplot2)
library(ggthemes)
library(flexdashboard)
library(plotly)
library(DT)
library(shiny)
library(knitr)
library(grid)
library(gridExtra)
library(tidymodels)
library(ggdendro)
library(kknn)
# optional, cluster trees should be prettier but not very interpretable 
library(heatmaply)
### package by Ashley
library(compmus)

### note to self: 
## 1. knit
## 2. Git, Staged, check
## 3. Commit with message
## 4. push
### ceptrogram
## can tell you how many timbre types there are, whether there is switching between timbres etc.
## beats are useful when the percussion is very clear, otherwise bars are great
### tempogram
## safe if you have something you like as an image
## or even safe dataframe/ggplot with saverds(), readrds()
### classification
## will give errors
```


Introduction {data-icon="ion-ios-home-outline"}
=========================================

Column {data-width=600}
-------------------------------------

### "The reason we're successful, darling? My overall charisma, of course." - Freddie Mercury. But was it really?

**Portfolio Computational Musicology**  
Although I am not a '70s kid, Queen is very nostalgic to me. I remember sitting in the backseat of the car, listening to my own CD with all the hits of the famous rock band. I could sing along to every song, even though I did not speak any English. The movie “Bohemian Rhapsody” portrayed (though not completely accurate) a small portion of the lives of the four band members. It reminded me of the solo careers three of the members had, only John Deacon never went solo. The drummer Roger Taylor was the first to release a solo album in 1981. After which Freddie Mercury and Brian May followed. When I heard the song [Time Waits For No One](https://open.spotify.com/track/0UkCqv8ec7D1W3uGzZa1CP?si=2b721e3080154085) for the first time not long ago, I thought it was just a Queen song I had not heard before. When I noticed that it was a song exclusively by Freddie Mercury, I wondered how it differed from songs produced by the band. Freddie Mercury has had less success as a solo artist than together with his three band members, what could have been the reason for this? 

Therefore, I will be comparing solo songs by Freddie Mercury with the songs from the band Queen. In addition to that, I will take a look at the solo songs by Roger Taylor as he was the first to go solo. There will also be some comparisons between Queen with Freddie Mercury as lead singer, and after the singer's death.

**Expectations**  
The band Queen is known for their grandiosity and experimentation. They were not afraid to deviate from the norm and tried to mix all kinds of genres together in their songs including rock, pop, metal, opera and electronic music. It is therefore very difficult to pinpoint the characteristics of Queen and to select typical songs. However, many songs seem to be theatrical or dramatic featuring intricate arrangements and harmonies, the lyrics have a message and explore complex themes, and the songs are pushing boundaries of rock music. These qualities can be clearly heard in what may be the most famous Queen song [Bohemian Rhapsody](https://open.spotify.com/track/4u7EnebtmKWzUH433cf5Qv?si=16ec60afbe5e4102), which includes the complex harmonies and shifts between different genres such as rock, pop and opera. It also beautifully displays Freddie Mercury's impressive range and theatrical feeling.

Needless to say, these iconic vocals can also be heard in the solo songs released by Freddie Mercury. He did seem to enjoy Disco music more than the other band members. Disco music is known for its great danceability. Therefore, the genre of disco music is characterized by a strong and steady beat, often resulting in a 4/4 time signature. To add more interest and to give songs more energy, guitar bass lines and drum hi-hats frequently contained syncopated rhythms. Disco music also uses a lot of different instruments and has repetitive but funky and soulful vocals. The classic disco beat can be heard for example in the songs [Love Kills](https://open.spotify.com/track/1cApo5IeHOea0dD3Cs3QMB?si=da3d27a25d344081), [Living on My Own](https://open.spotify.com/track/6cNTGVmP2CzJpLQ01QIaD0?si=63261b0fc1364d5f) and [I Was Born to Love You](https://open.spotify.com/track/14FAtIQZsRYeu8zVI33l7f?si=22236162e3fe4664) on his album Mr. Bad Guy. [Your Kind of Lover](https://open.spotify.com/track/2jzS0f1C4af20OavmCodWQ?si=f9879e78d4434b00) from the same album also includes a typical disco bassline.  
In addition to disco, opera is also a common genre in the songs released by Freddie Mercury. Together with opera singer Montserrat Caballé he has brought out an album, from which the like-named song [Barcelona](https://open.spotify.com/track/3BdId1EhcMX7yAzwzQJLAD?si=687c01b1ffe546e4) is most famous. These partially operatic songs will differ from the Queen songs in genre and pitch.  
Since Freddie Mercury was the chief songwriter of the band Queen, I expect lots of similarities in the songwriting such as catchy hooks and melodies, but I also anticipate Queen having more complex and grandiose songs while Freddie Mercury will probably be more stripped down and focused on vocals, piano and danceability.

**Representation**  
I have tried to make as complete a list as possible of the solo songs by Freddie Mercury. Nevertheless, there are a few non-original songs from the two albums he has released (*Mr. Bad Guy* and *Barcelona*) because those songs only are on Spotify as special editions (listed in *Song list 1.1*) or new orchestrated editions (listed in *Song list 1.2*). I have decided to include the songs to have a bigger set of data since there already are much fewer solo songs by Freddie Mercury (N = 25) in comparison to songs by the band Queen (N = 159).
Regarding the band, I have made a playlist with all albums going from 1973 till 1995. Each Queen song is remastered in 2011, while Spotify only states that [In My Defence](https://open.spotify.com/track/04L5RyQCKOerOiBCjhT66T?si=08106363d5cc4a0b) by Freddie Mercury is remixed in 2000. In *Song list 2.1* there are a few songs that stand out in the discography of Queen. In addition to these songs, the [tracks before *Killer Queen* was released](https://open.spotify.com/playlist/0Fgo3pP9bCEeamaSwYwIhQ) sound less clear than all songs that come after due to reverb on the vocals and guitar.

In addition, I have created a small playlist (N = 13) with the songs that were released by the band after the death of Freddie Mercury so I can investigate how Freddie Mercury's death affected the band. This playlist contains one track called [Yeah](https://open.spotify.com/track/65YeIE3Y4YBNLnXcpVZz1P?si=66ae769c6997402d) which is precisely what the title suggests: namely 4 seconds of Freddie Mercury singing "Yeah".

The playlist of songs by Roger Taylor consists of 44 songs from four different albums. 

*Song list 1.1: Special Editions Freddie Mercury*

  - Foolin' Around
  - Your Kind of Lover
  - Mr. Bad Guy
  - Man Made Paradise
  - There Must Be More to Life Than This

*Song list 1.2: New Orchestrated Editions Freddie Mercury*

  - La Japonaise
  - Ensueño
  - Guide Me Home

*Song list 2.1: Outliers Queen*

  - Ming's Theme
  - The Ring
  - The Hitman
  - Ogre Battle
  - '39

Column {data-width=200}
-------------------------------------

### Queen playlist
<iframe src="https://open.spotify.com/embed/playlist/3FOkKYT1wXXJH8L2lcSP5t?utm_source=generator" width="100%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

Column {data-width=200}
-------------------------------------

### Freddie Mercury playlist
<iframe src="https://open.spotify.com/embed/playlist/7IXPxFygCKwBEklOVopUu8?utm_source=generator" width="100%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>


General {.storyboard}
=========================================

### The most popular songs are danceable, happy and contain sung lyrics. {data-commentary-width=600}

```{r}
### first the tempo plots!!
# creating a color blind friendly color palette
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", 
"#CC79A7","#000000")
gray <- gray.colors(5, start = 0, end = 1, gamma = 2.2, rev = TRUE)
safe_palette <- c("#007B71", "#B6231B", "#977400", "#a9dea9", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")
palette2 <- c("#B6231B", "#977400", "#a9dea9", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")

freddie <-
  get_playlist_audio_features(
    "",
    "7IXPxFygCKwBEklOVopUu8"
  ) |> add_audio_analysis() %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
freddie$track.album.release_date = c(1987, 1992, 1984, 1988, 1985, 1985, 1988, 1989, 1985, 1985, 1988, 1988, 1985, 2019, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1988, 1988, 1988)
freddie <- freddie %>% filter(track.album.release_date != 2019)

queen <-
  get_playlist_audio_features(
    "",
    "3FOkKYT1wXXJH8L2lcSP5t"
  ) |> add_audio_analysis() %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

roger <-
  get_playlist_audio_features(
    "",
    "7eYTx9OtJPrKBst4bWr3PD"
  ) |> add_audio_analysis() %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

comp <-
  freddie |>
  mutate(category = "Freddie Mercury") |>
  bind_rows(queen |> mutate(category = "Band (Queen)"), roger |> mutate(category = "Roger Taylor"))

comp <- comp %>% rename(track = track.name)

Tempo <- comp |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections)

TempoPlot <- ggplot(Tempo,
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = category,
      alpha = factor(loudness),
      size = time_signature, 
      text = paste("Track: ", track, "<br>",
                          "Artist: ", category, "<br>",
                          "Release date: ", track.album.release_date, "<br>",
                          "Tempo: ", tempo, "<br>",
                          "Loudness: ", loudness, "<br>",
                          "Time signature: ", time_signature)
    )
  ) +
  geom_point() +
  geom_rug() +
  scale_color_manual(values = palette2) +
  scale_size_continuous(range = c(1, 6), breaks = seq(1, 6, by = 0.05), trans = "reverse") +
  scale_alpha_discrete(guide = "none") +
  annotate("text", x = 70, y = 4.8, label = "Queen") +
  annotate("text", x = 77, y = 4.3, label = "Freddie Mercury") +
  annotate("text", x = 74.2, y = 3.8, label = "Roger Taylor") +
  annotate("rect", xmin = 62 ,xmax = 64, ymin = 4.75 ,ymax = 4.89, fill = "#B6231B") +
  annotate("rect", xmin = 62 ,xmax = 64, ymin = 4.25 ,ymax = 4.39, fill = "#977400") +
  annotate("rect", xmin = 62 ,xmax = 64, ymin = 3.75 ,ymax = 3.89, fill = "#a9dea9") +
  theme_bw() +
  theme(legend.position='none') +
  ylim(0, 5) +
  labs(
    title = "Durational Features of Songs by Queen and Members of the Band",
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Artist",
    size = "Time Signature (beats)",
    alpha = "Volume (dBFS)"
  )
TempoPlotly <- ggplotly(TempoPlot, tooltip = "text")

# Animated Tempo Plot
TempoAnimated <- Tempo %>%
  plot_ly(x = ~tempo, y = ~tempo_section_sd, size = ~time_signature, color = ~track.album.release_date, opacity = ~factor(loudness), 
    hoverinfo = "text", text = paste("Track: ", Tempo$track, "<br>",
                          "Artist: ", Tempo$category, "<br>",
                          "Release date: ", Tempo$track.album.release_date, "<br>",
                          "Tempo: ", Tempo$tempo, "<br>",
                          "Loudness: ", Tempo$loudness, "<br>",
                          "Time signature: ", Tempo$time_signature)) %>%
  add_markers(frame = ~category) %>%
  layout(title = 'Tempo for Each Artist', xaxis = list(title = "Mean Tempo (bpm)"), yaxis = list(range = c(0,10), title = 'SD Tempo')) %>%
  animation_opts(frame = 2000, 
                 transition = 100,
                 easing = "linear") %>%
  animation_slider(currentvalue = list(prefix = NULL,
                                       font = list(color = "a9dea9", size = 20)))

### now the first plot begins

options(scipen=999)
options(digits = 3)

# my playlists
logo <- list.files("/Users/gebruiker/Documents/Studies/CLC/Jaar 2 2022-2023/Computational Musicology/Portfolio-Comp-Musicology", pattern = ".png", full.names = TRUE)
freddie <- get_playlist_audio_features('', '7IXPxFygCKwBEklOVopUu8') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
freddie$track.album.release_date = c(1987, 1992, 1984, 1988, 1985, 1985, 1988, 1989, 1985, 1985, 1988, 1988, 1985, 2019, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1988, 1988, 1988)

queen <- get_playlist_audio_features('', '3FOkKYT1wXXJH8L2lcSP5t') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

after <- get_playlist_audio_features('', '2Oou3UXp7fHZsHHxQ9jQnC') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
roger <- get_playlist_audio_features('', '7eYTx9OtJPrKBst4bWr3PD') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

Solo <- freddie %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Band <- queen %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Roger <- roger %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)

Comparison1 <-
  bind_rows(
    queen %>% mutate(category = "Band (Queen)"),
    freddie %>% mutate(category = "Freddie Mercury"),
    roger %>% mutate(category = "Roger Taylor")
  )
Comparison1 <- Comparison1 %>% rename(popularity = track.popularity, track = track.name)

Comparison2 <-
  bind_rows(
    queen %>% mutate(category = "Band (Queen)"),
    freddie %>% mutate(category = "Freddie Mercury"),
    after %>% mutate(category = "After the death"),
    roger %>% mutate(category = "Roger Taylor")
  )
Comparison2 <- Comparison2 %>% rename(popularity = track.popularity, track = track.name)

SumComparison <- 
    bind_rows(
    Band %>% mutate(category = "Band"),
    Solo %>% mutate(category = "Solo"),
    Roger %>% mutate(category = "Roger")
  )

plot1 <- ggplot(Comparison2, aes(x = tempo, y = danceability, fill = category, color = as.factor(time_signature))) +
  geom_point(shape = 21, size = 3.5, stroke = 1) +
  labs(y = 'Danceability', x = 'Tempo', fill = 'Artist', color = 'Beats per Measure') +
  scale_fill_manual(values=alpha((safe_palette), 0.6)) +
  scale_color_manual(values=gray) +
  scale_y_continuous(limits = c(0,1)) +
  geom_label(label="Yeah - Queen", x=25, y=0.005, label.padding = unit(0.20, "lines"), color = "black", fill = "white") +
  theme_minimal() +
  theme(legend.position = c(0.15, 0.6), axis.text.y = element_blank())

mean <- Comparison1 %>% 
  group_by(category) %>%
  summarize(meandanceability = mean(danceability))

plot3 <- ggplot(Comparison1, aes(x = danceability, y = popularity, color = valence, size = instrumentalness, text = paste("Track: ", track, "<br>",
                          "Artist: ", category, "<br>",
                          "Popularity: ", popularity, "<br>",
                          "Danceability: ", danceability, "<br>",
                          "Valence: ", valence, "<br>",
                          "Instrumentalness: ", instrumentalness))) +
  geom_vline(data= mean, aes(xintercept = meandanceability), linetype="dotted", linewidth = 0.3) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c() +
  labs(title = "The Danceability of Songs by Queen and Members of the Band", x = "Danceability", y = "Track Popularity", size = "Instrumentalness", color = "Valence") +
  facet_wrap(~category) +
  theme_bw() +
  theme(strip.background = element_rect(fill="#a9dea9"))

DanceabilityPlotly <- ggplotly(plot3, tooltip = "text")
DanceabilityPlotly

Dust <- Comparison1 %>% filter(track == "Another One Bites The Dust - Remastered 2011")
PopRoger <- Comparison1 %>% filter(track == "Man On Fire")
Forever <- Comparison1 %>% filter(track == "Who Wants To Live Forever - Remastered 2011")
```

***
**Popularity**  
The first thing that stands out is that Roger Taylor is far less popular than Queen and Freddie Mercury. The most popular song by Roger Taylor is [Man On Fire](https://open.spotify.com/track/6X2yqosH7UAcWZO07XN04e?si=fc8ac5d807ce4f6d) and has a popularity of `r PopRoger$popularity`. It is part of the album ‘Strange Frontier’ that was released in the UK in June 1984, with *Man On Fire* single released three weeks earlier. The three most popular songs by Queen are [Bohemian Rhapsody](https://open.spotify.com/track/4u7EnebtmKWzUH433cf5Qv?si=7427df9f9d464ba9), [Another One Bites The Dust](https://open.spotify.com/track/5vdp5UmvTsnMEMESIF2Ym7?si=4f3ba0d324524393) and [Don't Stop Me Now](https://open.spotify.com/track/5T8EDUDqKcs6OSOwEsfqG7?si=c365e244de0f4ca7) which all have a popularity of `r Dust$popularity`. Danceability and popularity seem to be correlated in Queen and Roger Taylor songs; the more danceable songs are also more popular. Within the songs of Freddie Mercury this pattern can not be found.

**Instrumentalness**  
As for instrumentalness, Spotify was not too precise, since the song with the highest score was [Seven Seas of Rhye](https://open.spotify.com/track/1IhLUUzMxDDJ9pzfT95exy?si=64be327348e141c3) which does have sung lyrics. The other songs at the top, though, are [The Ring (Hypnotic Seduction Of Dale)](https://open.spotify.com/track/5QTRYLNQY4eIiMCAwAUYpz?si=ff533253555347d4), [Vultan's Theme](https://open.spotify.com/track/5uJ6QwBbCSwomH4uUvG4Hp?si=66d18851eb714962), [God Save the Queen](https://open.spotify.com/track/2ISPUUp4pzssWuR2Ic09vR?si=daefd19a316d44bd) and [Escape From The Swamp](https://open.spotify.com/track/76nonU5KLNawCssAiENgGW?si=f1be613d64db4d1d) which are all completely instrumental. In the top 10 most instrumental songs, seven are from the album *Flash Gordon* which is an album made for the like-named science fiction movie. (If you want to know more about this album, check the [Queen official site](https://www.queenonline.com/music).) In the chromagram of the song [Ming's Theme](https://open.spotify.com/track/1kcsmxEal6ZbVF2N0FAVeT?si=1f141b030fe3472e) you will see how different these songs are from more typical Queen songs. It seems that the more instrumental a song is, the less popular it is on Spotify. There is one exception to this rule: *Another One Bites the Dust* is one of the most popular songs by Queen but has an instrumentalness of `r Dust$instrumentalness`. 

**Valence**  
The happiest song according to Spotify is [Misfire](https://open.spotify.com/track/1tLJMLavxD9KmeLTYeqnM8?si=0f18adba627b4756) by Queen. [Rain Must Fall](https://open.spotify.com/track/6jornLGJEzC3wFP2MFFvWg?si=811cc8228c3d4874) is following with only `r 0.001` points in difference. Both songs have a danceability score above the mean of `r Band$mean_danceability`. This seems to be fitting the norm; the happier a song is, the higher it rates on danceability. The lowest valence scores are from the songs *Ming's Theme* and *The Ring (Hypnotic Seduction Of Dale)* which also both are very instrumental. The next song with the lowest valence is a more typical song by the band, namely [Who Wants To Live Forever](https://open.spotify.com/track/3SGP8It5WDnCONyApJKRTJ?si=e1e6446a761646af). This song also has the lowest danceability score (`r Forever$danceability`).

### Oldest Queen songs deviate the most from the mean tempo, their recording skills probably have improved a lot. {data-commentary-width=500}

```{r}
TempoAnimated
```
***

**Explanation**  
Here you can see the mean tempo, the deviation of each song from that mean tempo, the loudness in opacity
and the beats per measure signature in the size of the point. The biggest deviation can be found in the oldest Queen songs, as the darker points are at the top and brighter points are more toward the bottom, or in the songs with either a relatively slow or fast tempo. Older Queen songs were recorded using analog equipment, which may have had limitations in terms of capturing precise tempos. This could have led to more variation in tempo in their older recordings. It might also be, that the earlier songs were even more experimental than later songs and incorporated more styles that suited more tempo changes. 

The song with the highest deviation in tempo by Freddie Mercury is a more classical song he released together with the opera singer Montserrat Caballé. In the tempogram you will also see how hard it is to extract the tempo using computation.

**Time signature**  
Spotify API had a hard time with the songs [Execution of Flash](https://open.spotify.com/track/5EhUncBbxhp117MVVkH5zw?si=08577c080e854099) as well as [Ming's Theme](https://open.spotify.com/track/1kcsmxEal6ZbVF2N0FAVeT?si=cd926f1047284e20) by Queen because there is no consistent time signature. The album *Flash Gordon* once again stands out, both of these songs are part of it, and as you will see in the chromagram section, this is not a typical album. The song [Dear Friends](https://open.spotify.com/track/2P2DVfvkVlh317i9PhFQqF?si=fcf560100d514f87) by Queen also stands out; it states that there is only one beat per measure but when I listened to the song, I felt like the time signature was 4/4. I do have to say that the arrangement with the piano emphasized all beats very equally, that might be an explanation for the weird time signature.

### After Freddie Mercury's death, the sadness could be felt in the songs by Queen. {data-commentary-width=500}

```{r}
mean2 <- Comparison2 %>% 
  group_by(category) %>%
  summarize(meanvalence = mean(valence), meanenergy = mean(energy))

plot2 <- ggplot(data = Comparison2) +
  geom_vline(xintercept = 0.5) +
  geom_hline(yintercept = 0.5) +
  geom_vline(data=mean2, aes(xintercept = meanvalence, color = category), linetype = "dotted", linewidth = 0.4) +
  geom_hline(data=mean2, aes(yintercept = meanenergy, color = category), linetype = "dotted", linewidth = 0.4) +
  geom_point(aes(x = valence, y = energy, color = category, text = paste(
    "Track: ", track, "<br>",
    "Artist: ", category, "<br>",
    "Energy: ", energy, "<br>",
    "Valence: ", valence, "<br>",
    "Key: ", key_mode)), size = 3, alpha = 0.7) +
  labs(y = 'Energy', x = 'Valence', color = 'Artist') +
  scale_x_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  annotate('text', 0.25 / 4, 1.05, label = "Turbulent/Angry", fontface = "bold") +
  annotate('text', 1-(0.25/4), 1.05, label = "Happy/Joyful", fontface = "bold") +
  annotate('text', 1-(0.25/4), -0.05, label = "Chill/Peaceful", fontface = "bold") +
  annotate('text', 0.25 / 4, -0.05, label = "Sad/Depressing", fontface = "bold") +
  #annotate(geom = "label", 0.75, 0.05, label = "Dear Friends - Queen", fill = "#a9dea9") +
  scale_color_manual(values=safe_palette) +
  theme_bw() +
  theme(legend.position = c(0.76, 0.23), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank(), panel.grid.major = element_blank())

EmotionPlotly <- ggplotly(plot2, tooltip = "text")
EmotionPlotly 
```
***
**Emotion Model**  
I was curious about the emotion every song portrays. Freddie Mercury has once said that he is very emotional. "I think all my songs are under the label emotion. The more I open up, the more I get hurt, so basically what happens is I'm just riddled with scars." After Freddie Mercury's death, I could see songs portraying more of a sad emotion. Since emotion is very complex, I have decided to simplify this by plotting the valence (positive and negative) against the arousal (for which I have selected high and low energy). This way, the songs are plotted on the 2D valence-arousal model of Emotion in which each quadrant stands for a different type of basic emotion (angry, happy, sad, peaceful).

**Explanation**  
There seem to be very few peaceful and calm pieces, which corresponds to the characteristics of the band. The only songs that are in this category are from the band with Freddie Mercury as lead singer. After the death of the singer, there seems to be a slight shift towards sad and angry songs. Roger Taylor has more turbulent songs, while Freddie Mercury also has quite a few sad songs. 


### Timbre features {data-commentary-width=500}
```{r}
freddie <-
  get_playlist_audio_features(
    "",
    "7IXPxFygCKwBEklOVopUu8"
  ) |>
  add_audio_analysis()

queen <-
  get_playlist_audio_features(
    "",
    "3FOkKYT1wXXJH8L2lcSP5t"
  ) |> slice(1:25) |>
  add_audio_analysis()

queen2 <-
  get_playlist_audio_features(
    "",
    "3FOkKYT1wXXJH8L2lcSP5t"
  ) |> slice(135:159) |>
  add_audio_analysis()

comp <-
  freddie |>
  mutate(category = "Freddie Mercury (1985)") |>
  bind_rows(queen |> mutate(category = "Queen (1973)"), queen2 |> mutate(category = "Queen (1990)"))

comp <- comp %>% rename(track = track.name)

comp |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(category, timbre) |>
  compmus_gather_timbre() |>
  ggplot(aes(x = basis, y = value, fill = category)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Artist") +
  theme_bw() +
  theme(legend.position = "bottom")
```

***

**Explanation**  
For every 12 levels of timbre used by Spotify API I have plotted the range for the songs by Freddie Mercury, the oldest songs by Queen (released around the year 1973) and the newest songs with Freddie Mercury still as lead singer (released around the year 1990). 

According to the clusters, some of the timbre levels should be very distinct between the three categories I plotted. For example, from most useful to less useful, the levels 4, 12, 3 and 1 should be quite different. Unfortunately, timbre is a very abstract term to talk about music. According to Jehan (2014) timbre "is the quality of a musical note or sound that distinguishes different types of musical instruments, or voices." Spotify uses 12 different levels to quantify timbre, the first of which is easy to understand, but that goes down very quickly. Level 1 represents the average loudness of a section and the second level calculates the difference in energy between lower and higher frequencies (this should represent the brightness, which is already much less concrete). The next dimension does this for the mid-frequencies (which should correlate to the flatness of the sound) and the fourth represents the energy of the attacks (the beginning of sounds).  
*(I still have to write a conclusion.)*

**Reference**  
Jehan, T., DesRoches, D. (2014). Analyzer documentation (analyzer version 3.2). Accessed 27 Mar. 2023

Chromagram {.storyboard}
=========================================

### Chromagram of a very atypical song by Queen (*Ming's Theme*) shows simplicity. {data-commentary-width=500}

```{r}
Born <-
  get_tidy_audio_analysis("14FAtIQZsRYeu8zVI33l7f") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Born <- Born %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "I Was Born To Love You - Freddie Mercury", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Stop <-
  get_tidy_audio_analysis("5T8EDUDqKcs6OSOwEsfqG7") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Stop <- Stop %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Don't Stop Me Now - Queen", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Ming <-
  get_tidy_audio_analysis("1kcsmxEal6ZbVF2N0FAVeT") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Ming <- Ming %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Ming's Theme - Queen", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Ming
```

***
**Chromagram**  
I have created three chromagrams to show the difference between a simple, atypical Queen song and complex Queen and Freddie Mercury songs that are very representative for their style. A chromagram shows the the energy for every pitch class during a song. 

**Explanation**  
A very atypical song by Queen is *Ming's Theme (In The Court Of Ming The Merciless)*. This song is part of the album that was made specifically for the movie *Flash Gordon*. It starts with low pitched electronic sounds and later on includes people talking. In the chromagram you can see how the song starts on a note that is somewhere around F sharp. There are two descending melodies leading to a note between C and C sharp. After this, the talking starts with the same note softly sounding in the background. Around two minutes into the song there is another descending melody. The melodic part ends with a perfect fifth (D and A). The song concludes with a dialogue from the movie.

**Comparison**  
On the next page the chromagram of *Don't Stop Me Now* and *I Was Born To Love You* can be seen. You will see that this clear distinction of notes and sections can not be made in a more typical Queen song, also in a Freddie Mercury song this is much harder. Even the key can not be easily identified.

<iframe src="https://open.spotify.com/embed/playlist/1jqc3j3k6VXu5RykFegI5x?utm_source=generator" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### Typical Queen and Freddie Mercury songs, however, are far from simple. {data-commentary-width=750}

```{r, fig.height=4, fig.width=5}
Stop
```

*** 
**Queen**  
For Queen, I think [Don't Stop Me Now](https://open.spotify.com/track/5T8EDUDqKcs6OSOwEsfqG7?si=5725ec384e4d4708) is a very representative song. It is very energetic, melodic and euphoric. It is also one of the most popular songs according to Spotify, along with *Bohemian Rhapsody* and *Another One Bites The Dust*.


```{r, fig.height=4.5, fig.width=5.5, out.extra='style="float:right; padding:10px"'}
Born
```
**Freddie Mercury**  
For Freddie Mercury as a solo artist I have chosen the song [I Was Born To Love You](https://open.spotify.com/track/14FAtIQZsRYeu8zVI33l7f?si=acd6af3a5c654d12), as it is the most popular song according to Spotify. I also think it is quite representative for Freddie Mercury, as it has a disco feeling and is very lively. The song *The Great Pretender* is equally popular, but is a song originally by The Platters and written by Buck Ram (their manager).  
I have also plotted *The Great Pretender* as a chromagram (can be seen on the next tab) and interestingly this was much more readable than original Queen and Freddie Mercury songs, just like *Ming's Theme*.

**Comparison**  
The chromagram shows that *Don't Stop Me Now* contains lots of pitch classes and it has thus very complex harmonies, while *I Was Born To Love You* seems to have a bit more of distinct pitch classes with high energy. The highest energy can be found in the class of G sharp, this is also the key of the song. For *Don't Stop Me Now* it is harder to see the key, though the pitch class G seems to have a slightly bigger magnitude than the other pitch classes, this song is in the key of F major.

### The chromagram of *The Great Pretender* originally from The Platters by Freddie Mercury. {data-commentary-width=500}

```{r}
Pretender <-
  get_tidy_audio_analysis("4xm2RpQtpFVlPlO1v4BrBr") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Pretender <- Pretender %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 126, color = "#B6231B", linewidth = 0.6) +
  labs(title = "The Great Pretender - Freddie Mercury", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Pretender
```

***
**Explanation**  
As said before, this song is originally by the Platters and covered by Freddie Mercury. In the chromagram you can clearly see that the song starts in G major from the frequent bright yellow stripes, meaning that there is most energy in the pitch class G, and modulates at the red line to G sharp major. The pitch classes C and D are also used repeatedly in the first 120 seconds. Freddie Mercury loved the song [The Great Pretender](https://open.spotify.com/track/4xm2RpQtpFVlPlO1v4BrBr?si=31835fb64c0e4364) for its meaning as he felt like he played different roles on stage; he went through different moods, wore different costumes and became someone else with each costume. 

**Conclusion**  
Both Queen and Freddie Mercury songs are known for the intricate harmonies, which the chromagrams seem to confirm. Though the solo work might be slightly less complex and sometimes a little more stripped down to focus on the rhythm and energy.

Self-similarity {.storyboard}
=========================================

### Queen songs tend to deviate from the clear structure that is frequent in popular music, unless Roger Taylor writes the song. {data-commentary-width=450}

```{r, fig.width=10, fig.height=9}
### different song
rhapsody <-
  get_tidy_audio_analysis("4u7EnebtmKWzUH433cf5Qv") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

car <-
  get_tidy_audio_analysis("253ewjP2R1LjKqA70mGdnk") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

CarPlot <- bind_rows(
  car |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  car |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  #theme(plot.title = element_text(size = 14)) +
  labs(x = "", y = "", title = "I'm in Love With My Car - Queen")

RhapsodyPlot <- bind_rows(
  rhapsody |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  rhapsody |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "", y = "", title = "Bohemian Rhapsody - Queen") +
  theme_minimal()

grid.arrange(CarPlot, RhapsodyPlot, ncol = 1)
```

*** 
**Self-similarity matrices**  
These are four self-similarity matrices, with the axes in seconds, that show the pitch- and timbre-based self-similarity for every bar. At the top you can see a song that is written and sung by the drummer Roger Taylor but released as a Queen song. On the bottom the song Bohemian Rhapsody, written by Freddie Mercury, can be seen. The song written by Roger Taylor seems to have much more of a uniform structure than [Bohemian Rhapsody](https://open.spotify.com/track/4u7EnebtmKWzUH433cf5Qv?si=bc21bcc1a0c04537), which can be derived from the checkerboard pattern.

**Explanation**  
The song [I'm in Love With My Car](https://open.spotify.com/track/253ewjP2R1LjKqA70mGdnk?si=644faee074ad4daf) starts with a guitar riff accompanied with repeating D chords on the piano, followed by verse 1 that makes use of four different chords that are repeated after one another. After 60 seconds, the chorus starts in which Freddie Mercury sings backing vocals. There is a slight change in color in the timbre-based matrix, but I expected to see a greater change. The chords stay the same, thus the transition from verse 1 to the chorus is not really visible in the chroma-based matrix. At around 1:20 minutes (80 seconds) into the song, the chords change. This can be clearly seen in the chroma-based matrix, but is less apparent in the timbre-based one. That the guitar riff and accompanying piano chords recur after this short change becomes evident from both matrices. Then there is another verse that follows the same structure as verse 1, but does include guitar riffs in the background. The two matrices pick up the repetition in the second chorus very well, resulting in diagonal lines at around 2:10 minutes (130 seconds). They play once again some guitar riffs accompanied by the repeating chords on the piano that slowly fade out with car noises sounding on top of that. The song ends with some chords on the guitar that sound very different (which explains the bright yellow line at the end of the timbre-based matrix) and car noises.

### Songs by Freddie Mercury as a solo artist also have a clearer structure. {data-commentary-width=450}

```{r}
livingown <-
  get_tidy_audio_analysis("5EyrSES4OYcUxpZdhO1MQj") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

LivingPlot <- bind_rows(
  livingown |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  livingown |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 11)) +
  labs(x = "", y = "", title = "Living on My Own - Freddie Mercury")
LivingPlot
```

***
**Comparison**  
As I explained in the introduction, Queen is known for their experimentation and deviation from the norm. On the previous tab this could be seen in the matrices of the song *Bohemian Rhapsody*, since there was some structure visible but not how you would expect a typical rock or pop song to be arranged. The song that was written by Roger Taylor had much more of a common and clear structure, as is the case with a very typical song by Freddie Mercury as a solo artist. On this page you can see the timbre- and pitch-based self-similarity matrices of the song [Living on My Own](https://open.spotify.com/track/6cNTGVmP2CzJpLQ01QIaD0?si=43805f08aca34fb0). The song is in my view a classic Freddie Mercury song, that features Freddie's distinctive voice, flamboyant personality, and his love for dance music. The song's upbeat tempo, catchy melody, and electronic production are all hallmarks of Freddie's solo work. Additionally, the song features a memorable chorus, which is typical of Freddie's ability to write catchy songs.

**Freddie Mercury**  
The chroma-based matrix illustrates nicely through the diagonal lines that there are lots of repetitions in the song. The songs starts with A minor chords and when the next chord is introduced, there is a yellow line in the chroma-based matrix (at around 25 seconds). At around 50 seconds, the chorus is introduced. In the same matrix it becomes clear that the chorus repeats after another 35 seconds (approximately 85 seconds into the song). Next there is a break with some scatting and improvisation on the piano, followed by halve of the chorus. The beat that is used throughout the whole song accompanied by piano chords and some occasional scatting are utilized as an outro. Though the true ending is the artificial repetition of one ad lib by Freddie Mercury.

### And here is an extra, interesting self-similarity matrice of an atypical song by Queen written by Brian May. {data-commentary-width=350}

```{r}
thirtynine <-
  get_tidy_audio_analysis("6aNP9GlBi3VHPXl7w3Qjr9") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

ThirtyninePlot <- bind_rows(
  thirtynine |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  thirtynine |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16)) +
  labs(x = "", y = "", title = "'39 - Queen")
ThirtyninePlot
```

***
**Explanation**  
Coincidentally, this song is also not written by Freddie Mercury but by Brian May. I found this song very interesting, because of it deviates greatly from the normal style of Queen. It is a blend of folk and progressive rock with acoustic instruments, vocal harmonies, and storytelling lyrics. At around 35 seconds into the song the beat comes in. This interestingly leads to a change in the chroma matrix but a much less clear change in the timbre matrix. After one and a half minutes the bridge introduces new chords. I find it very strange that this is so clear in the timbre matrix but not in the chroma matrix. It almost feels as though the matrices are switched around. Specially because the bright yellow cross in the chroma matrix is exactly where the electric guitar makes an appearance.

<iframe src="https://open.spotify.com/embed/playlist/2POKY9r3mluVe4rkDzS0AA?utm_source=generator" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### Queen songs can be structured, but they find a way to still make the song interesting. {data-commentary-width=350}

```{r, fig.width=12, fig.height=9}
innuendo <-
  get_tidy_audio_analysis("0BzhS74ByIVlyz8BedHaYi") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

InnuendoPlot <- bind_rows(
  innuendo |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  innuendo |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  #theme(plot.title = element_text(size = 9)) +
  labs(x = "", y = "", title = "Innuendo - Queen")

lovekills <-
  get_tidy_audio_analysis("1cApo5IeHOea0dD3Cs3QMB") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

LoveKillsPlot <- bind_rows(
  lovekills |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  lovekills |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  #theme(plot.title = element_text(size = 9)) +
  labs(x = "", y = "", title = "Love Kills - Freddie Mercury")

grid.arrange(LoveKillsPlot, InnuendoPlot, ncol = 1)
```

***
**Conclusion**  
Once again [this electronic dance song](https://open.spotify.com/track/1cApo5IeHOea0dD3Cs3QMB?si=3ebba557323543ea) by Freddie Mercury shows a noticeable structure with some diagonal lines indicating repetition, here of the chorus. 

The song [Innuendo](https://open.spotify.com/track/0BzhS74ByIVlyz8BedHaYi?si=4e3f4a5eac6149f9) by Queen, on the other hand, shows a very nice checkered pattern at the beginning and end, but there is a very different part in the middle. I think this is a good summary of Queen; some song will have a clear structure but they will always find a way to still make the song interesting, whereas Freddie Mercury's songs are sometimes a little more simple and rhythmical. I also wanted to note that besides the structure of the song by Queen, the vocal performance is also very impressive which is typical for Freddie Mercury in general. "[Innuendo] would prove to be the last album Freddie worked upon and yet also, despite his palpably deteriorating health, a work containing some of his most powerful and emotive vocal performances of all." as [Queen said themselves](https://www.queenonline.com/music). Definitely have a listen if you are not familiar with the song!

Tempo {.storyboard}
=========================================

### The seemingly structured song *Innuendo* (according to the self-similarity matrices) has a difficult tempo to detect. {data-commentary-width=450}

```{r}
readRDS("InnuendoTempogram")
```

***

**Tempogram**  
A tempogram shows for every possible tempo how well it matches with the song. A good match turns up bright yellow and everywhere you see dark blue, the tempo does not lign up with the song.

**Explanation**  
In the self-similarity matrices I ended with the song [Innuendo](https://open.spotify.com/track/0BzhS74ByIVlyz8BedHaYi?si=0a5663f08a594ea0) by Queen. Even though the song has a very clear drum and rhythm, the drum does make use of triplets which probably makes it more difficult to detect a uniform tempo. When the very contrasting bridge starts (at around 2:45), they start with a low energy guitar with hints of flamenco along with soft vocals by Freddie Mercury. In this section the rhythm sometimes slows down or speeds up to match the dynamics of the vocals and other instruments, also known as rubato. Despite the fluctuation in tempo, this section does seem to have a lower tempo which can also be seen in the figure (there are more bright yellow bits between 80 and 120 BPM then before). Just before the 200 seconds mark, the speed picks up and you hear guitar and clapping. Brian May incorporates Spanish guitar techniques such as tremolo picking and arpeggios, which are commonly found in Mediterranean and flamenco music. Flamenco music is also very rhythmic; as you can see in the graph, this part has a much clearer tempo though still not a very consistent tempo. Another section starts at around 236 seconds, which is a sort of classical, operatic, walz part.  
This tempogram shows that despite the seemingly clear structure the song has according to the self-similarity matrices, there is definitely a lot going on in terms of rhythm and tempo.

### The song that was written by Roger Taylor has a clearer tempo in the tempogram. 

```{r}
CarTempogram <- readRDS("CarTempogram")
CarTempogram
```

***
**Explanation**  
[This](https://open.spotify.com/track/253ewjP2R1LjKqA70mGdnk?si=eb97228f37284740) clearly structured song (as could be seen in the self-similarity matrices) written by Roger Taylor has faintly more of a clear tempo. The uptempo rock song has a driving rhythm and features Taylor's vocals and a prominent guitar riff from Brian May. The lyrics are about Taylor's love for his car and as I said before, the song also includes some car sounds, but that did not change the accuracy of the tempogram.

As you shall see, the tempo might be more clear than the tempo in the previous song because the song is more focused on the percussion, but it is certainly not as unambigious as the following electronic songs by Freddie Mercury.


### The disco songs by Freddie Mercury are much easier to plot in a tempogram. 

```{r}
Kills <- readRDS("LoveKillsTempogram")
Living <- readRDS("LivingOwnTempogram")
Born <- readRDS("BornTempogram")
Lover <- readRDS("YourKindofLoverTempogram")

grid.arrange(Kills, Living, Born, Lover, ncol = 2)
```

***

**Explanation**  
All these songs are solo songs by Freddie Mercury. The bottom two are part of the album *Mr. Bad Guy*, the other two are singles. The songs are very electronic and disco inspired. As [Freddie Mercury said himself](http://www.freddiemercury.com/en/archive#mr_bad_guy): "I wanted to cover such things as reggae rhythms and I’ve done a couple of things with a symphony orchestra. It has a very rich sound and it’s very beat orientated." Electronic, beat orientated songs such as these four are very likely to have a clearly visible tempo. Thus, I was also curious about a very different song, namely *Barcelona*, which you will see in the next tab.

### The song *Barcelona* by Freddie Mercury and opera singer Montserrat Caballé is all over the place tempo wise. 

```{r}
readRDS("BarcelonaTempogramCyc")
```

***
**Explanation**  
The song [Barcelona](https://open.spotify.com/track/3BdId1EhcMX7yAzwzQJLAD?si=7cee17fa0b0943e7) is very "un-rock ’n’ roll" as Freddie Mercury stated himself, just like the rest of the album. It "really required a lot of discipline" and "[the] ideas [...] underwent significant changes along the road to perfection." ([Freddie Mercury's official site](http://www.freddiemercury.com/en/archive#Barcelona_original)). In other words, Freddie Mercury put a lot of time and effort in the songs on the album *Barcelona* to create something really unique. The album incorporates lots of classical elements with more traditional pop characteristics. Classical music often features multiple layers of melody and rhythm that interact in complex ways, and this can make it difficult to separate out the different elements of the music and plot them in a coherent tempogram. This can be seen in the tempogram of the song *Barcelona*.


### The distribution of tempi for each artist deviates from the preferred tempo. {data-commentary-width=450}

```{r}
TempiPlot <- ggplot(Comparison2, aes(tempo, fill = category, text = paste("Artist: ", category, "<br>",
                                                                          "Tempo: ", tempo, "<br>", 
                                                                          "Album: ", track.album.name))) +
  geom_histogram(binwidth = 5) +
  scale_fill_manual(values=safe_palette) +
  annotate("text", x = 5, y = 2, label = "Yeah - Queen", size = 3) +
  annotate("text", x = 202.5, y = 4, label = "My Life Has", size = 3) +
  annotate("text", x = 202.5, y = 3, label = "Been Saved", size = 3) +
  annotate("text", x = 202.5, y = 2, label = "- Queen", size = 3) +
  labs(title = "Distribution of Tempi", x = "Tempo", y = "Frequency", fill = "Artist", fontface = "bold", size = 2) +
  theme_bw() +
  theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank())

ggplotly(TempiPlot, tooltip = "text")
```

***
**Conclusion**  
Interestingly, even though according to [Dirk Moelants (2002)](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0762d815903c643b587250cfc1f4659c9da9c4a5) the preferred tempo is between 120 and 130 beats per minute (BPM), in this plot you can see that most songs are around 80 BPM. This is relatively slow and specially for the many disco inspired songs by Freddie Mercury. The most frequent tempo is around 110 BPM for Queen, and 160 BPM for Roger Taylor, but Queen has a more normal distribution and the drummer's songs are very evenly distributed.

I think we can say that Queen songs are overall a little hard to plot in a tempogram (which also ties back to the amount of standard deviation in the general tempo plot) while Freddie Mercury songs either have a very easy tempo to detect, as in his electronic disco songs, or songs in which the tempo is more complicated to plot in a tempogram, such as the more classical songs.  

**Reference**  
Moelants, D. (2002). Preferred tempo reconsidered. C. Stevens, D. Burnham, G. McPherson, E. Schubert, J. Renwick (Eds.), *Proceedings of the 7th International Conference on Music Perception and Cognition*. Sydney, Adelaide, Causal Productions, 580–583.

Chords {.storyboard}
=========================================

### Two chordograms of balads: *Love of My Life* and *Time Waits for No One*. {data-commentary-width=450}

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

major_key <-
  c(5.0, 2.0, 3.5, 2.0, 4.5, 4.0, 2.0, 4.5, 2.0, 3.5, 1.5, 4.0)
minor_key <-
  c(5.0, 2.0, 3.5, 4.5, 2.0, 4.0, 2.0, 4.5, 3.5, 2.0, 1.5, 4.0)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )


time <-
  get_tidy_audio_analysis("0UkCqv8ec7D1W3uGzZa1CP") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

loveofmylife <-
  get_tidy_audio_analysis("2BlNyI35idBaI6BN6WGZeQ") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

turn <-
  get_tidy_audio_analysis("6pDnUKu9z2FKTmwS7TDm3Q") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

rhapsody <-
  get_tidy_audio_analysis("4u7EnebtmKWzUH433cf5Qv") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

TimePlot <- time |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  scale_y_discrete(guide = guide_axis(n.dodge=2)) +
  theme_minimal() +
  labs(title = "Time Waits for No One - Freddie Mercury", x = "", y = "") +
  theme(axis.text.y = element_text(size = 7), 
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        plot.margin = unit(c(5.5, 5.5, 0, 5.5), "pt"))

LovePlot <- loveofmylife |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  scale_y_discrete(guide = guide_axis(n.dodge=2)) +
  theme_minimal() +
  labs(title = "Love of My Life - Queen", x = "Time (s)", y = "") +
  theme(axis.text.y = element_text(size = 7), plot.margin=unit(c(5.5, 5.5, 0, 5.5), "pt"))

TurnPlot <- turn |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Let's Turn it On - Freddie Mercury", x = "Time (s)", y = "")

RhapsodyChordPlot <- rhapsody |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Bohemian Rhapsody - Queen", x = "Time (s)", y = "")

grid.arrange(TimePlot, LovePlot, ncol = 1)
```

***
*(I have to shorten this explanation)*  
**Chordogram**  
These chordograms show the similarity for every bar of a song with certain chords. The more similar the bar is to that specific chord, the darker the color in the graph. When you see bright yellow, it is very unlikely that chords was played in that bar. 

The two songs are, in my opinion, quite similar in style. Both ballads have intricate piano arrangements and are a little more stripped down compared to the rest of the discography. That is why I thought it would be interesting to see how the chordograms compared to one another. 

**Time Waits for No One**  
Even though the plot looks quite clear, it is still harder to get all the right chords since Freddie Mercury composed a complex piano arrangement that has multiple chords (and definitely not only simple triads or triads with the added 7th note) in one bar. What makes it easier then *Love of My Life* is that he only uses the piano and his voice.

**Love of My Life**  
At first glance it looks a little messy to me. There are four very distinct yellow lines and some darker blue boxes. After listening to the song while taking a closer look at the graph I discovered that the start of the song was most clearest. The song starts with an instrumental introduction that ends with an arpeggio G major chord on the harp which can also be seen by the dark blue box at 20 seconds. When Freddie Mercury starts singing (after the arpeggio) the accompanying chords are C, Am, Dm, G and then C, F, Dm, Am, B♭m, F which I could also find in the figure. However, after the first yellow line, which can be explained by the piano riff that is played, the plot gets a little more muddy. There is another instrumental bit that focuses more on melody than on triads and the second yellow line is created by the waling bass line. At around 1 minute and 47 seconds, the third yellow line, there is a guitar riff preceded by piano riffs with walking bass lines. From 2 minutes and 20 seconds you can hear a guitar solo. At 2:27 (fourth yellow line) there is one sustained guitar note (with some piano notes accompanying). After 193 seconds there is another arpeggio that results in a dark blue box on F major.

**Comparison**  
There are four very bright lines in the Queen plot, which could explain why the rest of the figure is less saturated. But after taking a listening closely to the song, those parts did not stand out too much in my opinion. Another explanation could be, and I think is therefore more likely, that the song by Freddie Mercury is less complicated than the Queen song.

### Electronic disco song *Let's Turn it On* by Freddie Mercury leaves weird pattern on all 7 chords. {data-commentary-width=450}

```{r}
TurnPlot
```

***

**Explanation**  
I think it is interesting to see that in this very electronic song, there is not a clear pattern in the chords, whereas in the former two figures there was some sort of dark blue path throughout the plots. All chords that include the 7 have a darker blue line across the entire plot, which is explainable by the way pitches are constructed. 

### Chordogram of *Bohemian Rhapsody*. {data-commentary-width=450}

```{r}
RhapsodyChordPlot
```

***

**Explanation**  
Since *Bohemian Rhapsody* is such a complex song, I wondered what it would look like in a chordogram. I will also still take a look at it in more detail.

**Conclusion**  
*(Still have to write this.)*

Keys {.storyboard}
=========================================

### After the death of Freddie Mercury the band released most songs in G major, just like Freddie Mercury in his solo career.
```{r}
KeysRatio <- Comparison2 %>%
  group_by(category, key_mode) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ungroup()
KeysPlotRatio <- ggplot(KeysRatio, aes(key_mode, freq, fill = category, text = paste("Artist: ", category, "<br>",
                          "Key: ", key_mode, "<br>",
                          "Percentage: ", freq, "<br>",
                          "Actual frequency:", n))) +
  geom_col(position = position_dodge(preserve = "single", width = 0.8), alpha = 0.8, width = 1) +
  scale_fill_manual(values=safe_palette) +
  labs(title = "Distribution of Keys", x = "Key", y = "Frequency in Probability", fill = "Artist", fontface = "bold", angle = 90, size = 2) +
  theme_bw() +
  theme(legend.position = c(0.76, 0.23), axis.text.x = element_text(angle = 90), axis.ticks.x = element_blank(), axis.ticks.y = element_blank())

roger <-
  get_playlist_audio_features(
    "",
    "7eYTx9OtJPrKBst4bWr3PD"
  ) |>
  add_audio_analysis()

after <-
  get_playlist_audio_features(
    "",
    "2Oou3UXp7fHZsHHxQ9jQnC"
  ) |>
  add_audio_analysis()

comp2 <-
  after |> mutate(category = "After") |>
  bind_rows(freddie |> mutate(category = "Freddie"), roger |> mutate(category = "Roger"), queen |> mutate(category = "Band (Queen)"))

KeysPlot <- ggplot(KeysRatio, aes(key_mode, n, fill = category, text = paste("Artist: ", category, "<br>",
                          "Key: ", key_mode, "<br>",
                          "Actual frequency:", n))) +
  geom_col(width = 1) +
  scale_fill_manual(values=safe_palette) +
  labs(title = "Distribution of Keys", x = "Key", y = "Frequency", fill = "Artist", fontface = "bold", angle = 90, size = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90), axis.ticks.x = element_blank(), axis.ticks.y = element_blank(), strip.background = element_rect(fill="#a9dea9")) +
  facet_wrap(~category)

KeysPlotlyRatio <- ggplotly(KeysPlotRatio, tooltip = "text")
KeysPlotlyRatio

NTracks <- Comparison2 %>% group_by(category) %>% summarise(Tracks = n()) %>% rename(Artist = category)
```
***

**Explanation**  
Here you can see the distribution of keys for every artist. In the next tab you can see the actual amounts (which can also be seen in if you hover over the points), but as there is such a difference in playlist length (see the table), I have chosen to first plot the ratio.

Interestingly, the key G major was very loved by Freddie Mercury, Roger Taylor and the band after the death of the lead singer. Nevertheless, before that, the band used the key D major the most frequent.

**Numbers of Tracks per Artist**  
`r NTracks %>% knitr::kable()`

### Distribution of keys for every song in actual amounts.
```{r}
ggplotly(KeysPlot, tooltip = "text")
```
***

*(Still have to write a conclusion.)*

**Numbers of Tracks per Artist**  
`r NTracks %>% knitr::kable()`


Clusters {.storyboard}
=========================================

### Freddie Mercury produced operatic pop, electronic disco and experimental pop music. {data-commentary-width=450}

```{r, fig.width=10}
freddie <-
  get_playlist_audio_features("", "7IXPxFygCKwBEklOVopUu8") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

freddie$album.name = c("Cover", "For Musical", "First solo release", "Barcelona", "Single", "Mr. Bad Guy", "Barcelona", "Single", "Mr. Bad Guy", "Mr. Bad Guy", "Barcelona", "Barcelona", "Mr. Bad Guy", "Single", "Single", "Single", "Mr. Bad Guy", "Mr. Bad Guy", "Mr. Bad Guy", "Mr. Bad Guy", "Mr. Bad Guy", "Mr. Bad Guy", "Barcelona", "Barcelona", "Barcelona")

freddie_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration,
    data = freddie
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(freddie |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")

freddie_dist <- dist(freddie_juice, method = "manhattan")

data_for_freddie_clustering <- freddie_dist |> 
  hclust(method = "complete") |> # average for a balanced tree!
  dendro_data() 

playlist_data_for_join <- freddie %>%
  select(track.name, album.name) %>%
  mutate(label = str_trunc(track.name, 20))

data_for_freddie_clustering$labels <- data_for_freddie_clustering$labels %>%
  left_join(playlist_data_for_join)

# Add factor so can use colouring! 
data_for_freddie_clustering$labels$label <- factor(data_for_freddie_clustering$labels$label)

treeFreddieColor <- data_for_freddie_clustering |>
  ggdendrogram() +
  geom_text(data = label(data_for_freddie_clustering), aes(x, y, 
                                   label=label, 
                                   hjust=0, 
                                   colour=album.name), size=3) +
  coord_flip() + 
  scale_color_viridis_d() +
  scale_y_reverse(expand=c(0.2, 0)) +
  theme(axis.line.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank(),
        axis.title.y=element_blank(),
        panel.background=element_rect(fill="white"),
        panel.grid=element_blank()) +
  labs(title = "Freddie Mercury Clustering") +
  guides(
    colour = guide_legend(
      title = "Playlist"
    )
  )
treeFreddieColor
```

***
**Dendrogram**  
In a dendrogram, items that are the most similar get clustered together. The longer the paths and thus the further the connection, the less similar the items or clusters are. I have used all features that are within the Spotify API to cluster Freddie Mercury's solo songs.

**Freddie Mercury**  
I like how you can see the more classical songs, the electronic disco songs and the remaining songs clustered. Each album seems to have their own distinct sound. [She Blows Hot and Cold](https://open.spotify.com/track/6I3giXXPjsOXDWz5CF2q2D?si=686476eb612f46bf) stands out, it is kind of the only outlier. It is much more generic pop-rock and definitely not like the electronic songs on the *Mr. Bad Guy* album. The song [In my Defence](https://open.spotify.com/track/04L5RyQCKOerOiBCjhT66T?si=75047a0784bb4326) was not written by Freddie Mercury. Dave Clark wrote it for the musical Time but the song still suits the singer very well. I am not surprised it was clustered together with the song [Made in Heaven](https://open.spotify.com/track/54duq94ybKaMCB5Fj2UciX?si=b4d1a06b16054667), as this is as much of a grand, theatrical and powerful song. 

**Queen and Roger Taylor**  
I have also checked the clusters of Roger Taylor and Queen - because of the big amount of tracks, both are very cluttered and thus I decided not to include them - which had much less clear clusters than the dendrogram of Freddie Mercury. However, one album by Queen very much stood out; *Flash Gordon* was almost completely isolated from the rest of the tracks. The two most similar songs by Queen were *These are the Days of our Lives* and
*Delilah*. After listening to the songs, I can see the similarity in timbre but not much further than that. The two songs *'39* and *Leaving Home Ain't Easy* were also clustered together. Both songs are written and sung by Brian May and have a lot of country influences. 


### Clustering early Queen, late Queen and Freddie Mercury leads to three main clusters. 

```{r, fig.height=8, fig.width=10}
queen <- get_playlist_audio_features("", "3FOkKYT1wXXJH8L2lcSP5t")

freddie <- get_playlist_audio_features("", "7IXPxFygCKwBEklOVopUu8")

Comparison4 <-
  bind_rows(
    freddie |> mutate(artist = "Freddie Mercury") |> slice_head(n = 25),
    queen |> mutate(artist = "Queen 1973") |> slice_head(n = 25), 
    queen |> mutate(artist = "Queen 1990") |> tail(n = 25)
  ) |> 
  add_audio_analysis()

Comparison4[35, "track.name"] = "First Seven Seas Of Rhye - Remastered 2011"

## specially for clustering, be more specific in the recipe
comparison_juice <-
  recipe(
    track.name ~ # label you want to predict, in clustering that is always tracks
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration,
      #C + `C#|Db` + D + `D#|Eb` +
      #E + `F` + `F#|Gb` + G +
      #`G#|Ab` + A + `A#|Bb` + B +
      #c01 + c02 + c03 + c04 + c05 + c06 +
      #c07 + c08 + c09 + c10 + c11 + c12,
    data = Comparison4
  ) |>
  step_center(all_predictors()) |> 
  step_scale(all_predictors()) |>    #leads to Z score, with euclidean in dist()
  #step_range(all_predictors()) |>  #clamps between 0 and 1, then with manhattan distance in dist()
  prep(Comparison4 |> mutate(track.name = str_trunc(track.name, 20))) |> #cuts track names to 20 characters
  juice() |>
  column_to_rownames("track.name")

# computing distances
comparison_dist <- dist(comparison_juice, method = "manhattan")

data_for_comp_clustering <- comparison_dist |> 
  hclust(method = "complete") |> # average for a balanced tree!
  dendro_data() 

playlist_data_for_join <- Comparison4 %>%
  select(track.name, artist) %>%
  mutate(label = str_trunc(track.name, 20))

data_for_comp_clustering$labels <- data_for_comp_clustering$labels %>%
  left_join(playlist_data_for_join)

# Add factor so can use colouring! 
data_for_comp_clustering$labels$label <- factor(data_for_comp_clustering$labels$label)

treeCompColor <- data_for_comp_clustering |>
  ggdendrogram() +
  geom_text(data = label(data_for_comp_clustering), aes(x, y, 
                                   label=label, 
                                   hjust=0, 
                                   colour=artist), size=3) +
  coord_flip() + 
  scale_color_viridis_d() +
  scale_y_reverse(expand=c(0.2, 0)) +
  theme(axis.line.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank(),
        axis.title.y=element_blank(),
        panel.background=element_rect(fill="white"),
        panel.grid=element_blank()) +
  labs(title = "Artist Clustering") +
  guides(
    colour = guide_legend(
      title = "Artist"
    )
  )
treeCompColor

```

***
**Explanation**  
In this dendrogram you can find all 25 tracks by Freddie Mercury I put in the playlist compared to the first and last 25 songs by Queen using all the features Spotify API has to offer. 

*Brighton Rock* looks a bit like an outlier, as well as *Procession* (both . But overall, most Freddie Mercury songs are clustered together and the earlier Queen songs too. The oldest Queen songs are a bit mixed together with Freddie Mercury solo songs. 

**Conclusion**  
*(Still have to write this.)*


Classification {.storyboard}
=========================================

### Telling Freddie Mercury apart from Queen is not very hard. 

```{r}
# classifier
freddie <- get_playlist_audio_features("spotify", "7IXPxFygCKwBEklOVopUu8")
queen <- get_playlist_audio_features("spotify", "3FOkKYT1wXXJH8L2lcSP5t")
roger <- get_playlist_audio_features("spotify", "7eYTx9OtJPrKBst4bWr3PD")
after <- get_playlist_audio_features("spotify", "2Oou3UXp7fHZsHHxQ9jQnC")

# toevoegen |> slice_head(n = 20)
Comparison4 <-
  bind_rows(
    freddie |> mutate(artist = "Freddie Mercury") |> slice_head(n = 25),
    queen |> mutate(artist = "Queen 1973") |> slice_head(n = 25), 
    queen |> mutate(artist = "Queen 1990") |> tail(n = 25)
  ) |> 
  add_audio_analysis()

comp_features <-
  Comparison4 |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    artist = factor(artist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

comp_recipe <-
  recipe(
    artist ~
      danceability +
      energy +
      loudness +
      speechiness +
      #acousticness +
      #instrumentalness +
      #liveness +
      #valence +
      #tempo +
      duration,
      #C + `C#|Db` + D + `D#|Eb` +
      #E + `F` + `F#|Gb` + G +
      #`G#|Ab` + A + `A#|Bb` + B +
      #c01 + c02 + c03 + c04 + c05 + c06 +
      #c07 + c08 + c09 + c10 + c11 + c12,
    data = comp_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

comp_cv <- comp_features |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
comp_knn <- 
  workflow() |> 
  add_recipe(comp_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(comp_cv, control = control_resamples(save_pred = TRUE))

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 

#comp_knn |> get_conf_mat()
comp_knn |> get_conf_mat() |> autoplot(type = "heatmap")

```

***
**Confidence matrix**  
Based on the features danceability, energy, loudness, speechiness and duration, the computer tried to make a model to predict whether a song was by Freddie Mercury, early Queen or late Queen. These features were the most important besides some timbre features. I once again put in all 25 songs by Freddie Mercury and the first and last 25 songs by Queen. Here you can see how well the model did; the number in the cells show for every combination (the three true categories and how the three categories were predicted) how often this was the case. For example, for every song by Freddie Mercury, the model predicted 20 times correctly and only once that is was an early Queen song and four times that is was a late Queen song. Classifying Freddie Mercury seems to be the easiest. But both the earliest and latest songs by Queen were apparently also not too hard to classify, since the model predicted most songs right. This could also be expected based on how the dendrogram on the previous page already seemed to cluster most of the songs by artist somewhat right. 


### Plot

```{r}
library(ranger)
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
comp_forest <- 
  workflow() |> 
  add_recipe(comp_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    comp_cv, 
    control = control_resamples(save_pred = TRUE)
  )
features <- workflow() |> 
  add_recipe(comp_recipe) |> 
  add_model(forest_model) |> 
  fit(comp_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")

Plot <- comp_features |>
  ggplot(aes(x = c04, y = loudness, colour = artist, size = danceability, text = track.name, alpha = speechiness)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    title = "Distribution over Most Important Features", 
    x = "Timbre Component 4",
    y = "Loudness",
    size = "Danceability",
    colour = "Artist", 
    alpha = "Speechiness"
  ) +
  theme_bw()
ggplotly(Plot, tooltip = "text")
```

***
**Explanation**  
The most important feature to tell Freddie Mercury apart from Queen is timbre level 4, which shows the energy in attacks of sounds. Outside of the timbre features, loudness, danceability and speechiness were most important.
*(Still have to write a conclusion.)*


### Classifying Queen after the death of Freddie Mercury is much harder. 

```{r}
# classifier
freddie <- get_playlist_audio_features("spotify", "7IXPxFygCKwBEklOVopUu8")
queen <- get_playlist_audio_features("spotify", "3FOkKYT1wXXJH8L2lcSP5t")
roger <- get_playlist_audio_features("spotify", "7eYTx9OtJPrKBst4bWr3PD")
after <- get_playlist_audio_features("spotify", "2Oou3UXp7fHZsHHxQ9jQnC")

# toevoegen |> slice_head(n = 20)
comp <-
  bind_rows(
    freddie |> mutate(artist = "Freddie Mercury") |> slice_head(n = 25),
    queen |> mutate(artist = "Queen 1973") |> slice_head(n = 25), 
    queen |> mutate(artist = "Queen 1990") |> tail(n = 25), 
    roger |> mutate(artist = "Roger Taylor") |> slice_head(n = 25), 
    after |> mutate(artist = "Queen after Freddie")
  ) |> 
  add_audio_analysis()

comp_features <-
  comp |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    artist = factor(artist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

comp_recipe <-
  recipe(
    artist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = comp_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

comp_cv <- comp_features |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
comp_knn <- 
  workflow() |> 
  add_recipe(comp_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(comp_cv, control = control_resamples(save_pred = TRUE))

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 

#comp_knn |> get_conf_mat()
comp_knn |> get_conf_mat() |> autoplot(type = "heatmap")

```

***
**Explanation**  
I also wanted to know whether Roger Taylor could be as easily classified as Freddie Mercury and the band. Once again, Freddie Mercury was classified very well and the same holds for early Queen. Later Queen and Roger Taylor were a little less accurately classified, but not nearly as bad as Queen after the death of the singer. The few songs that were released in honor of Freddie Mercury have more in common with the solo songs by both Freddie Mercury and Roger Taylor than the songs by Queen as a band. 
*(Still have to give the accuracy score.)*


### Plot

```{r}
library(ranger)
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
comp_forest <- 
  workflow() |> 
  add_recipe(comp_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    comp_cv, 
    control = control_resamples(save_pred = TRUE)
  )
features <- workflow() |> 
  add_recipe(comp_recipe) |> 
  add_model(forest_model) |> 
  fit(comp_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")

Plot <- comp_features |>
  ggplot(aes(x = c09, y = c04, colour = artist, size = loudness, text = track.name)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    title = "Distribution over Most Important Features", 
    x = "Timbre Component 9",
    y = "Timbre Component 4",
    size = "Loudness",
    colour = "Artist"
  ) +
  theme_bw()
ggplotly(Plot, tooltip = "text")
```

***
**Explanation**  
In terms of timbre, the groups are quite distinguishable as you can see in the plot and in the former tab. As I have mentioned before, what this exactly means is very hard to say as timbre is very abstract. Spotify uses 12 different components to calculate timbre, which get more abstract the higher the level. Therefore, you can probably imagine that timbre component 9 is very incomprehensible, in fact, there is not even a description of it. 

**Conclusion**  
*(Still have to write this.)*


Conclusion {data-icon="ion-android-bulb"}
=========================================

Column {data-width=400}
-------------------------------------

### “I won’t be a rock star. I’ll be a legend.” - Freddie Mercury.

`r knitr::include_graphics(logo)`

Column {data-width=600}
-------------------------------------

### A legend he is and here is why.

I expected lots of similarities between the songs by Freddie Mercury and the band Queen; catchy hooks and melodies, dramatic and energetic vocal arrangements. But I also anticipated Queen having more complex and grandiose songs while Freddie Mercury's discography would probably be more stripped down and focused on vocals, piano and danceability.

From the plots that I made we can see that there is a lot of overlap. Nevertheless, Queen has much more popular songs than Freddie Mercury as a solo artist, which are mostly upbeat, danceable and include lyrics. Queen incorporates complex harmonies within the high energy songs. Freddie Mercury's songs are also sophisticated but not all popular songs are also danceable. Both queen and Freddie Mercury in his solo career were trying to explore new styles and tried to step out of the box. While Queen did that even within a single song, Freddie Mercury did that more so across his different albums. In other words, opera, rock, pop and electronic music are all part of their styles, but Queen combined them all at once and one of their most popular songs is a great example of that, *Bohemian Rhapsody*. The singer separated them more; the disco music, with elements of electronic and synth-pop music can be found on his album *Mr. Bad Guy* and the theatrical, operatic and classical style with hints of pop and rock on the following album *Barcelona*. 

Queen, of course, did not only consist of the singer Freddie Mercury. The other band members brought their own unique musical influences and perspectives to the creative processes. Songs by Roger Taylor for example were characterized by a more rock-oriented sound and clearer structure, with an emphasis on percussion. Brian May incorporated elements of country and folk music into the mix. Each member brought their own musical expertise and interests to the band, resulting in a diverse range of styles and sounds in their music. The collaborative nature of the band allowed for the integration of each member's musical ideas, ultimately leading to their distinctive sound and success as a group.
