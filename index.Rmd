---
title: "Computational Musicology"
author: "Bente Westermann"
date: 'februari 2023'
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: default
---

```{r, setup}
# loading packages
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(ggthemes)
library(flexdashboard)
library(plotly)
library(DT)
library(shiny)
library(knitr)
### package by Ashley
library(compmus)

### note to self: 
## 1. knit
## 2. Git, Staged, check
## 3. Commit with message
## 4. push
```

### Welcome to exploration of Queen {data-commentary-width=600}

**Portfolio Computational Musicology**

Although I am not a '70s kid, Queen is very nostalgic to me. I remember sitting in the backseat of the car, listening to my own CD with all the hits of the famous rock band. I could sing along to every song, even though I did not speak any English. The movie “Bohemian Rhapsody” portrayed (though not completely accurate) a small portion of the lives of the four band members. It reminded me of the solo careers three of the members had, only John Deacon never went solo. The drummer Roger Taylor was the first to release a solo album in 1981. After which Freddie Mercury and Brian May followed. When I heard “Time Waits For No One” for the first time not long ago, I thought it was just a Queen song I had not heard before. When I noticed that it was a song exclusively by Freddie Mercury, I wondered how it differed from songs produced by the band. Freddie Mercury has had less success as a solo artist than together with his three band members. 

Therefore, I will be comparing solo songs by Freddie Mercury with the songs from the band Queen. In addition to that, I am curious whether his solo career had any influence on the songs that were produced afterwards together with the band members. 

**Representation**

I have tried to make as complete a list as possible of the solo songs by Freddie Mercury. Nevertheless, there are a few non-original songs from the two albums he has released (*Mr. Bad Guy* and *Barcelona*) because those songs only are on Spotify as special editions (listed in *Song list 1.1*) or new orchestrated editions (listed in *Song list 1.2*). I have decided to include the songs to have a bigger set of data since there already are much fewer solo songs by Freddie Mercury (N = 25) in comparison to songs by the band Queen (N = 159).
Regarding the band, I have made a playlist with all albums going from 1973 till 1995. Each Queen song is remastered in 2011, while Spotify only states that *In My Defence* by Freddie Mercury is remixed in 2000. 

In addition, I have created a small playlist (N = 13) with the songs that were released by the band after the death of Freddie Mercury so I can investigate how Freddie Mercury's death affected the band. This playlist contains one track called *Yeah* which is precisely what the title suggests: namely 4 seconds of Freddie Mercury singing "Yeah".

Song list 1.1: Special Editions

  - Foolin' Around
  - Your Kind of Lover
  - Mr. Bad Guy
  - Man Made Paradise
  - There Must Be More to Life Than This

Song list 1.2: New Orchestrated Editions

  - La Japonaise
  - Ensueño
  - Guide Me Home

***

<iframe src="https://open.spotify.com/embed/playlist/3FOkKYT1wXXJH8L2lcSP5t?utm_source=generator" width="50%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/7IXPxFygCKwBEklOVopUu8?utm_source=generator" width="50%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### First plot

```{r}
# my playlists
freddie <- get_playlist_audio_features('', '7IXPxFygCKwBEklOVopUu8')
queen <- get_playlist_audio_features('', '3FOkKYT1wXXJH8L2lcSP5t')
after <- get_playlist_audio_features('', '2Oou3UXp7fHZsHHxQ9jQnC')
roger <- get_playlist_audio_features('', '7eYTx9OtJPrKBst4bWr3PD')

Solo <- freddie %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Band <- queen %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Roger <- roger %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)

Comparison1 <-
  bind_rows(
    queen %>% mutate(category = "Band"),
    freddie %>% mutate(category = "Solo"),
    roger %>% mutate(category = "Roger")
  )

Comparison2 <-
  bind_rows(
    queen %>% mutate(category = "Band"),
    freddie %>% mutate(category = "Solo"),
    after %>% mutate(category = "After")
  )

Comparison3 <-
  bind_rows(
    queen %>% mutate(category = "Band"),
    after %>% mutate(category = "After")
  )

SumComparison <- 
    bind_rows(
    Band %>% mutate(category = "Band"),
    Solo %>% mutate(category = "Solo"),
    Roger %>% mutate(category = "Roger")
  )

popQueen <- queen %>% 
  arrange(desc(track.popularity)) %>%
  slice(1:10)

popFreddie <- freddie %>% 
  arrange(desc(track.popularity)) %>%
  slice(1:10)

popRoger <- roger %>% 
  arrange(desc(track.popularity)) %>%
  slice(1:10)

popComparison <-
  bind_rows(
    popQueen %>% mutate(category = "Band"),
    popFreddie %>% mutate(category = "Solo"),
    popRoger %>% mutate(category = "Roger")
  )
# creating a color blind friendly color palette
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", 
"#CC79A7","#000000")
gray <- gray.colors(5, start = 0, end = 1, gamma = 2.2, rev = TRUE)

plot1 <- ggplot(Comparison2, aes(x = tempo, y = danceability, fill = category, color = as.factor(time_signature))) +
  geom_point(shape = 21, size = 3.5, stroke = 1) +
  labs(y = 'Danceability', x = 'Tempo', fill = 'Category', color = 'Beats per Measure') +
  scale_fill_manual(values=alpha((cbbPalette), 0.6)) +
  scale_color_manual(values=gray) +
  scale_y_continuous(limits = c(0,1)) +
  geom_label(label="Yeah - Queen", x=25, y=0.005, label.padding = unit(0.20, "lines"), color = "black", fill = "white") +
  theme_minimal() +
  theme(legend.position = c(0.15, 0.6), axis.text.y = element_blank())


plot2 <- ggplot(data = Comparison2, aes(x = valence, y = energy, fill = category)) +
  geom_point(shape = 21, size = 3, alpha = 0.6, stroke =0.5) +
  labs(y = 'Energy', x = 'Valence', fill = 'Category') +
  geom_vline(xintercept = 0.5) +
  geom_hline(yintercept = 0.5) +
  scale_x_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  annotate('text', 0.25 / 4, 1.05, label = "Turbulent/Angry", fontface = "bold") +
  annotate('text', 1-(0.25/4), 1.05, label = "Happy/Joyful", fontface = "bold") +
  annotate('text', 1-(0.25/4), -0.05, label = "Chill/Peaceful", fontface = "bold") +
  annotate('text', 0.25 / 4, -0.05, label = "Sad/Depressing", fontface = "bold") +
  scale_fill_manual(values=cbbPalette) +
  theme_minimal() +
  theme(legend.position = c(0.76, 0.23), axis.text.x = element_blank(), axis.text.y = element_blank())

ggplotly(plot2)
```

***
Since Freddie Mercury was inspired by disco music when he released his solo tracks, I was curious whether there would be a difference in danceability between the band, Freddie Mercury as solo artist and after Freddie Mercury's death. I will also investigate whether it has changed after the solo career,  when Freddie Mercury was still the lead singer.

There is only one song by Freddie Mercury with 3 beats per measure, the remaining songs are in 4/4. Queen has 20 songs that deviate from the mode. There does not seem to be a clear correlation between tempo and danceability. It might be interesting to include more rhythm aspects into this comparison, such as duration. Also, energy and instrumentalness might influence the danceability.

Besides danceability, I was also curious about the emotion every song portrays. Specially after Freddie Mercury's death, I could see songs portraying more of a sad emotion. Since emotion is very complex, I have decided to simplify this by plotting the valence (positive and negative) against the arousal (for which I have selected high and low energy). This way, the songs are plotted on the 2D valence-arousal model of Emotion in which each quadrant stands for a different type of basic emotion (angry, happy, sad, peaceful).

There seem to be very few peaceful and calm pieces, which corresponds to the characteristics of the band. The only songs that are in this category are from the band with Freddie Mercury as lead singer. After the death of the singer, there seems to be a slight shift towards sad and angry songs. 


### This is a spectogram {data-commentary-width=800}

```{r, out.extra='style="float:right"'}
Pretender <-
  get_tidy_audio_analysis("4xm2RpQtpFVlPlO1v4BrBr") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Pretender <- Pretender %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "The Great Pretender", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

Stop <-
  get_tidy_audio_analysis("5T8EDUDqKcs6OSOwEsfqG7") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Stop <- Stop %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Don't Stop Me Now", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

Ming <-
  get_tidy_audio_analysis("1kcsmxEal6ZbVF2N0FAVeT") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Ming <- Ming %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Ming's Theme", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

Pretender
```

*** 
```{r, out.extra='style="float:right; padding:10px"'}
Stop
```

I have plotted the spectograms of both Freddie Mercury as a solo artist and Queen the band. For the plot of Freddie Mercury as a solo artist I have chosen the song *The Great Pretender*, as it is the most popular according to Spotify. 

For Queen, I think *Don't Stop Me Now* is a very representative song by the band Queen. It is very energetic, melodic and euphoric. It is also one of the most popular songs according to Spotify, along with *Bohemian Rhapsody* and *Another One Bites The Dust*.

The spectogram shows that *Don't Stop Me Now* contains lots of pitch classes and is has thus very complex harmonies, while *The Great Pretender* seems to have a more simple harmony throughout the song. From Freddie Mercury's spectogram it becomes clear that the song *The Great Pretender* is in the key G major and modulates to G# major at around 2 minutes into the song.

A very different song by Queen is *Ming's Theme*. It starts with low electronic sounds and later on 


### Spectogram Made In Heaven

```{r}
SoloHeaven <-
  get_tidy_audio_analysis("54duq94ybKaMCB5Fj2UciX") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

BandHeaven <-
  get_tidy_audio_analysis("4NTMIFWtDXnWN4hDSBlKOf") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

compmus_long_distance(
  SoloHeaven %>% mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  BandHeaven %>% mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  feature = pitches,
  method = "manhattan"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Freddie Mercury", y = "Queen after the death of Freddie Mercury") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)
```
