---
title: "It's a kind of magic"
author: "Bente Westermann"
date: 'februari 2023'
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      bg: "#fffffa"
      fg: "#1c1b1b" 
      primary: "#8fbc8f"
      navbar-bg: "#8fbc8f"
      base_font: 
        google: Source Sans Pro
      heading_font:
        google: Sen
---

```{r, setup, echo=FALSE}
# loading packages
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(ggthemes)
library(flexdashboard)
library(plotly)
library(DT)
library(shiny)
library(knitr)
library(grid)
library(gridExtra)
### package by Ashley
library(compmus)

### note to self: 
## 1. knit
## 2. Git, Staged, check
## 3. Commit with message
## 4. push
### ceptrogram
## can tell you how many timbre types there are, whether there is switching between timbres etc.
## beats are useful when the percussion is very clear, otherwise bars are great
```

### Queen songs tend to deviate from the clear structure that is frequent in popular music, unless Roger Taylor writes the song. {data-commentary-width=450}
```{r, fig.width=10, fig.height=9}
### different song
rhapsody <-
  get_tidy_audio_analysis("4u7EnebtmKWzUH433cf5Qv") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

car <-
  get_tidy_audio_analysis("253ewjP2R1LjKqA70mGdnk") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

CarPlot <- bind_rows(
  car |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  car |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  #theme(plot.title = element_text(size = 14)) +
  labs(x = "", y = "", title = "I'm in Love With My Car")

RhapsodyPlot <- bind_rows(
  rhapsody |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  rhapsody |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "", y = "", title = "Bohemian Rhapsody") +
  theme_minimal()

grid.arrange(CarPlot, RhapsodyPlot, ncol = 1)
```

***
_(Newest plots on first 2 pages)_  
**Self-similarity matrices**  
These are four self-similarity matrices, with the axes in seconds, that show the pitch- and timbre-based self-similarity for every bar. At the top you can see a song that is written and sung by the drummer Roger Taylor but released as a Queen song. On the bottom the song Bohemian Rhapsody, written by Freddie Mercury, can be seen. The song written by Roger Taylor seems to have much more of a uniform structure than *Bohemian Rhapsody*, which can be derived from the checkerboard pattern.

**Explanation**  
The song *I'm in Love With My Car* starts with a guitar riff accompanied with repeating D chords on the piano, followed by verse 1 that makes use of four different chords that are repeated after one another. After 60 seconds, the chorus starts in which Freddie Mercury sings backing vocals. There is a slight change in color in the timbre-based matrix, but I expected to see a greater change. The chords stay the same, thus the transition from verse 1 to the chorus is not really visible in the chroma-based matrix. At around 1:20 minutes (80 seconds) into the song, the chords change. This can be clearly seen in the chroma-based matrix, but is less apparent in the timbre-based one. That the guitar riff and accompanying piano chords recur after this short change becomes evident from both matrices. Then there is another verse that follows the same structure as verse 1, but does include guitar riffs in the background. The two matrices pick up the repetition in the second chorus very well, resulting in diagonal lines at around 2:10 minutes (130 seconds). They play once again some guitar riffs accompanied by the repeating chords on the piano that slowly fade out with car noises sounding on top of that. The song ends with some chords on the guitar that sound very different (which explains the bright yellow line at the end of the timbre-based matrix) and car noises.

### Songs by Freddie Mercury as a solo artist also have a clearer structure. {data-commentary-width=450}

```{r}
livingown <-
  get_tidy_audio_analysis("5EyrSES4OYcUxpZdhO1MQj") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

LivingPlot <- bind_rows(
  livingown |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  livingown |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 11)) +
  labs(x = "", y = "", title = "Living on My Own")
LivingPlot
```

***
**Comparison**  
As I explained in the introduction, Queen is known for their experimentation and deviation from the norm. On the previous tab this could be seen in the matrices of the song *Bohemian Rhapsody*, since there was some structure visible but not how you would expect a typical rock or pop song to be arranged. The song that was written by Roger Taylor had much more of a common and clear structure, as is the case with a very typical song by Freddie Mercury as a solo artist. On this page you can see the timbre- and pitch-based self-similarity matrices of the song *Living on My Own*. The song is in my view a classic Freddie Mercury song, that features Freddie's distinctive voice, flamboyant personality, and his love for dance music. The song's upbeat tempo, catchy melody, and electronic production are all hallmarks of Freddie's solo work. Additionally, the song features a memorable chorus, which is typical of Freddie's ability to write catchy songs.

**Freddie Mercury**  
The chroma-based matrix illustrates nicely through the diagonal lines that there are lots of repetitions in the song. The songs starts with A minor chords and when the next chord is introduced, there is a yellow line in the chroma-based matrix (at around 25 seconds). At around 50 seconds, the chorus is introduced. In the same matrix it becomes clear that the chorus repeats after another 35 seconds (approximately 85 seconds into the song). Next there is a break with some scatting and improvisation on the piano, followed by halve of the chorus. The beat that is used throughout the whole song accompanied by piano chords and some occasional scatting are utilized as an outro. Though the true ending is the artificial repetition of one ad lib by Freddie Mercury.

### "The reason we're successful, darling? My overall charisma, of course." - Freddie Mercury. But was it really? {data-commentary-width=600}

**Portfolio Computational Musicology**  
Although I am not a '70s kid, Queen is very nostalgic to me. I remember sitting in the backseat of the car, listening to my own CD with all the hits of the famous rock band. I could sing along to every song, even though I did not speak any English. The movie “Bohemian Rhapsody” portrayed (though not completely accurate) a small portion of the lives of the four band members. It reminded me of the solo careers three of the members had, only John Deacon never went solo. The drummer Roger Taylor was the first to release a solo album in 1981. After which Freddie Mercury and Brian May followed. When I heard the song *Time Waits For No One* for the first time not long ago, I thought it was just a Queen song I had not heard before. When I noticed that it was a song exclusively by Freddie Mercury, I wondered how it differed from songs produced by the band. Freddie Mercury has had less success as a solo artist than together with his three band members, what could have been the reason for this? 

Therefore, I will be comparing solo songs by Freddie Mercury with the songs from the band Queen. In addition to that, I am curious whether his solo career had any influence on the songs that were produced afterwards together with the band members. I will also take a look at the solo songs by Roger Taylor as he was the first to go solo. At the end of my page, there will also be a small comparison between Queen with Freddie Mercury as lead singer, and after the singer's death.

**Expectations** 
The band Queen is known for their grandiosity and experimentation. They were not afraid to deviate from the norm and tried to mix all kinds of genres together in their songs including rock, pop, metal, opera and electronic music. It is therefore very difficult to pinpoint the characteristics of Queen and to select typical songs. However, many songs seem to be theatrical or dramatic featuring intricate arrangements and harmonies, the lyrics have a message and explore complex themes, and the songs are pushing boundaries of rock music. These qualities can be clearly heard in what may be the most famous Queen song *Bohemian Rhapsody*, which includes the complex harmonies and shifts between different genres such as rock, pop and opera. It beautifully displays Freddie Mercury's impressive range and theatrical feeling.

Needless to say, these iconic vocals can also be heard in the solo songs released by Freddie Mercury. He did seem to enjoy Disco music more than the other band members. Disco music is known for its great danceability. Therefore, the genre of disco music is characterized by a strong and steady beat, often resulting in a 4/4 time signature. To add more interest and to give songs more energy, guitar bass lines and drum hi-hats frequently contained syncopated rhythms. Disco music also uses a lot of different instruments and has repetitive but funky and soulful vocals. The classic disco beat can be heard for example in the songs *Love Kills*, *Living on My Own* and *I Was Born to Love You* on his album Mr. Bad Guy. *Your Kind of Lover* from the same album also includes a typical disco bassline.  
In addition to disco, opera is also a common genre in the songs released by Freddie Mercury. Together with opera singer Montserrat Caballé he has brought out an album, from which the song with the same name as the album, *Barcelona*, is most famous. These partially operatic songs will differ from the Queen songs in genre and pitch.  
Since Freddie Mercury was the chief songwriter of the band Queen, I expect lots of similarities in the songwriting such as catchy hooks and melodies, but I also anticipate Queen having more complex and grandiose songs while Freddie Mercury will probably be more stripped down and focused on vocals, piano and danceability.

**Representation**  
I have tried to make as complete a list as possible of the solo songs by Freddie Mercury. Nevertheless, there are a few non-original songs from the two albums he has released (*Mr. Bad Guy* and *Barcelona*) because those songs only are on Spotify as special editions (listed in *Song list 1.1*) or new orchestrated editions (listed in *Song list 1.2*). I have decided to include the songs to have a bigger set of data since there already are much fewer solo songs by Freddie Mercury (N = 25) in comparison to songs by the band Queen (N = 159).
Regarding the band, I have made a playlist with all albums going from 1973 till 1995. Each Queen song is remastered in 2011, while Spotify only states that *In My Defence* by Freddie Mercury is remixed in 2000. In *Song list 2.1* there are a few songs that stand out in the discography of Queen. In addition to these songs, the tracks from before *Killer Queen* sound less clear than all songs that come after due to reverb on the vocals and guitar.

In addition, I have created a small playlist (N = 13) with the songs that were released by the band after the death of Freddie Mercury so I can investigate how Freddie Mercury's death affected the band. This playlist contains one track called *Yeah* which is precisely what the title suggests: namely 4 seconds of Freddie Mercury singing "Yeah".

The playlist of songs by Roger Taylor consists of 44 songs from four different albums. 

Song list 1.1: Special Editions

  - Foolin' Around
  - Your Kind of Lover
  - Mr. Bad Guy
  - Man Made Paradise
  - There Must Be More to Life Than This

Song list 1.2: New Orchestrated Editions

  - La Japonaise
  - Ensueño
  - Guide Me Home

Song list 2.1

  - Ming's Theme
  - The Ring
  - The Hitman
  - Ogre Battle
  - '39

***

<iframe src="https://open.spotify.com/embed/playlist/3FOkKYT1wXXJH8L2lcSP5t?utm_source=generator" width="50%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/7IXPxFygCKwBEklOVopUu8?utm_source=generator" width="50%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### The most popular songs are danceable, happy and have sung lyrics. {data-commentary-width=600}

```{r}
options(scipen=999)
options(digits = 3)

# my playlists
logo <- list.files("/Users/gebruiker/Documents/Studies/CLC/Jaar 2 2022-2023/Computational Musicology/Portfolio-Comp-Musicology", pattern = ".png", full.names = TRUE)
freddie <- get_playlist_audio_features('', '7IXPxFygCKwBEklOVopUu8') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
freddie$track.album.release_date = c(1987, 1992, 1984, 1988, 1985, 1985, 1988, 1989, 1985, 1985, 1988, 1988, 1985, 2019, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1988, 1988, 1988)

queen <- get_playlist_audio_features('', '3FOkKYT1wXXJH8L2lcSP5t') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
after <- get_playlist_audio_features('', '2Oou3UXp7fHZsHHxQ9jQnC') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
roger <- get_playlist_audio_features('', '7eYTx9OtJPrKBst4bWr3PD') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

Solo <- freddie %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Band <- queen %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Roger <- roger %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)

Comparison1 <-
  bind_rows(
    queen %>% mutate(category = "Band (Queen)"),
    freddie %>% mutate(category = "Freddie Mercury"),
    roger %>% mutate(category = "Roger Taylor")
  )
Comparison1 <- Comparison1 %>% rename(popularity = track.popularity, track = track.name)

Comparison2 <-
  bind_rows(
    queen %>% mutate(category = "Band (Queen)"),
    freddie %>% mutate(category = "Freddie Mercury"),
    after %>% mutate(category = "After the death"),
    roger %>% mutate(category = "Roger Taylor")
  )
Comparison2 <- Comparison2 %>% rename(popularity = track.popularity, track = track.name)

SumComparison <- 
    bind_rows(
    Band %>% mutate(category = "Band"),
    Solo %>% mutate(category = "Solo"),
    Roger %>% mutate(category = "Roger")
  )

# creating a color blind friendly color palette
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", 
"#CC79A7","#000000")
gray <- gray.colors(5, start = 0, end = 1, gamma = 2.2, rev = TRUE)
safe_palette <- c("#88CCEE", "#CC6677", "#DDCC77", "#117733", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")

plot1 <- ggplot(Comparison2, aes(x = tempo, y = danceability, fill = category, color = as.factor(time_signature))) +
  geom_point(shape = 21, size = 3.5, stroke = 1) +
  labs(y = 'Danceability', x = 'Tempo', fill = 'Category', color = 'Beats per Measure') +
  scale_fill_manual(values=alpha((safe_palette), 0.6)) +
  scale_color_manual(values=gray) +
  scale_y_continuous(limits = c(0,1)) +
  geom_label(label="Yeah - Queen", x=25, y=0.005, label.padding = unit(0.20, "lines"), color = "black", fill = "white") +
  theme_minimal() +
  theme(legend.position = c(0.15, 0.6), axis.text.y = element_blank())

plot3 <- ggplot(Comparison1, aes(x = danceability, y = popularity, color = valence, size = instrumentalness, key = track)) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c() +
  labs(title = "The Danceability of Songs by Queen and Members of the Band", x = "Danceability", y = "Track Popularity", size = "Instrumentalness", color = "Valence") +
  facet_wrap(~category) +
  theme_bw() +
  theme(strip.background = element_rect(fill="#a9dea9"))

ggplotly(plot3, source = "select", tooltip = c("key", "x", "y", "color", "size"))

Dust <- Comparison1 %>% filter(track == "Another One Bites The Dust - Remastered 2011")
PopRoger <- Comparison1 %>% filter(track == "Man On Fire")
Forever <- Comparison1 %>% filter(track == "Who Wants To Live Forever - Remastered 2011")
```

***
**Popularity**  
The first thing that stands out is that Roger Taylor is far less popular than Queen and Freddie Mercury. The most popular song by Roger Taylor is *Man On Fire* and has a popularity of `r PopRoger$popularity`. The three most popular songs by Queen are *Bohemian Rhapsody*, *Another One Bites The Dust* and *Don't Stop Me Now* which all have a popularity of `r Dust$popularity`. Danceability and popularity seem to be correlated in Queen and Roger Taylor songs; the more danceable songs are also more popular. Within the songs of Freddie Mercury this pattern can not be found.

**Instrumentalness**  
As for instrumentalness, Spotify was not too precise, since the song with the highest score was *Seven Seas of Rhye* which does have sung lyrics. The other songs at the top, though, are *The Ring*, *Vultan's Theme*, *God Save the Queen* and *Escape From The Swamp* which are all completely instrumental. In the top 10 most instrumental songs, seven are from the album *Flash Gordon* which is an album made for the homonymous science fiction movie. It seems that the more instrumental a song is, the less popular it is on Spotify. There is one exception to this rule: *Another One Bites the Dust* is one of the most popular songs by Queen but has an instrumentalness of `r Dust$instrumentalness`. 

**Valence**  
The happiest song according to Spotify is *Misfire* by Queen. *Rain Must Fall* is following with only `r 0.001` points in difference. Both songs have a danceability score above the mean of `r Band$mean_danceability`. This seems to be fitting the norm; the happier a song is, the higher it rates on danceability. The lowest valence scores are from the songs *Ming's Theme* and *The Ring (Hypnotic Seduction Of Dale)* which also both are very instrumental. The next song with the lowest valence is a more typical song by the band, namely *Who Wants To Live Forever*. This song also has the lowest danceability score (`r Forever$danceability`).

### Chromagram of a very atypical song by Queen (*Ming's Theme*) shows simplicity. {data-commentary-width=600}

```{r}
Born <-
  get_tidy_audio_analysis("14FAtIQZsRYeu8zVI33l7f") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Born <- Born %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "I Was Born To Love You - Freddie Mercury", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Stop <-
  get_tidy_audio_analysis("5T8EDUDqKcs6OSOwEsfqG7") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Stop <- Stop %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Don't Stop Me Now - Queen", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Ming <-
  get_tidy_audio_analysis("1kcsmxEal6ZbVF2N0FAVeT") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Ming <- Ming %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Ming's Theme - Queen", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Ming
```

***
**Chromagram**  
I have created three chromagrams to show the difference between a simple and atypical Queen song and complex Queen and Freddie Mercury songs that are very representative for their style. A chromagram shows the the energy for every pitch class during a song. 

**Explanation**  
A very atypical song by Queen is *Ming's Theme (In The Court Of Ming The Merciless)*. This song is part of the album that was made specifically for the movie *Flash Gordon*. It starts with low pitched electronic sounds and later on includes people talking. In the chromagram you can see how the song starts on a note that is somewhere around F sharp. There are two descending melodies leading to a note between C and C sharp. After this, the talking starts with the same note softly sounding in the background. Around two minutes into the song there is another descending melody. The melodic part ends with a perfect fifth (D and A). The song concludes with a dialogue from the movie.

**Comparison**  
On the next page the chromagram of *Don't Stop Me Now* and *I Was Born To Love You* can be seen. You will see that this clear distinction of notes and sections can not be made in a more typical Queen song, also in a Freddie Mercury song this is much harder. Even the key can not be easily identified.

<iframe src="https://open.spotify.com/embed/playlist/1jqc3j3k6VXu5RykFegI5x?utm_source=generator" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### Typical Queen and Freddie Mercury songs, however, are far from simple. {data-commentary-width=750}

```{r, fig.height=4, fig.width=5}
Stop
```

*** 
**Queen**  
For Queen, I think *Don't Stop Me Now* is a very representative song. It is very energetic, melodic and euphoric. It is also one of the most popular songs according to Spotify, along with *Bohemian Rhapsody* and *Another One Bites The Dust*.


```{r, fig.height=4.5, fig.width=5.5, out.extra='style="float:right; padding:10px"'}
Born
```
**Freddie Mercury**  
For Freddie Mercury as a solo artist I have chosen the song *I Was Born To Love You*, as it is the most popular song according to Spotify. I also think it is quite representative for Freddie Mercury, as it has a disco feeling and is very lively. The song *The Great Pretender* is equally popular, but is a song originally by The Platters and written by Buck Ram (the manager).  
I have also plotted *The Great Pretender* as a chromagram (can be seen on the second to last page) and interestingly this was much more readable than original Queen and Freddie Mercury songs, just like *Ming's Theme*. In the chromagram you can clearly see that the song is in G major and modulates at around 2 minutes to G sharp major.

**Comparison**  
The chromagram shows that *Don't Stop Me Now* contains lots of pitch classes and it has thus very complex harmonies, while *I Was Born To Love You* seems to have a bit more of distinct pitch classes with high energy. The highest energy can be found in the class of G sharp, this is also the key of the song. For *Don't Stop Me Now* it is harder to see the key, but the pitch class F seems to have a slightly bigger magnitude than the other pitch classes. This songs is in the key of F major.

### “I won’t be a rock star. I’ll be a legend.” - Freddie Mercury. A legend he is and here is why. {data-commentary-width=450}

`r knitr::include_graphics(logo)`

***

**Conclusion**  
From the plots that I made we can already see that there is a lot of overlap between the songs by Freddie Mercury and the band Queen. Nevertheless, Queen has much more popular songs than Freddie Mercury as a solo artist. The popular songs are mostly happy, danceable and include lyrics. Queen has made use of complex harmonies and the songs are high in energy. Freddie Mercury's songs are also complex but not all popular ones are also danceable. 

I will look more into durational features in the next couple of weeks as rhythm is a big part of what is  characteristic to disco music. Disco music was beloved by Freddie Mercury, but not by the other three band members. Thus, I am curious to see whether there is a difference that could help explain the distinction between the band Queen and Freddie Mercury.


### After Freddie Mercury's death, the sadness could be felt in the songs by Queen. {data-commentary-width=500}

```{r}
plot2 <- ggplot(data = Comparison2) +
  geom_vline(xintercept = 0.5) +
  geom_hline(yintercept = 0.5) +
  geom_point(aes(x = valence, y = energy, color = category, key = track), size = 3, alpha = 0.7) +
  labs(y = 'Energy', x = 'Valence', color = 'Category') +
  scale_x_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  annotate('text', 0.25 / 4, 1.05, label = "Turbulent/Angry", fontface = "bold") +
  annotate('text', 1-(0.25/4), 1.05, label = "Happy/Joyful", fontface = "bold") +
  annotate('text', 1-(0.25/4), -0.05, label = "Chill/Peaceful", fontface = "bold") +
  annotate('text', 0.25 / 4, -0.05, label = "Sad/Depressing", fontface = "bold") +
  #annotate(geom = "label", 0.75, 0.05, label = "Dear Friends - Queen", fill = "#a9dea9") +
  scale_color_manual(values=safe_palette) +
  theme_bw() +
  theme(legend.position = c(0.76, 0.23), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank())

ggplotly(plot2, source = "select", tooltip = c("key", "x", "y", "color"))
```
***
**Emotion Model**  
I was curious about the emotion every song portrays. Freddie Mercury has once said that he is very emotional. "I think all my songs are under the label emotion. The more I open up, the more I get hurt, so basically what happens is I'm just riddled with scars." After Freddie Mercury's death, I could see songs portraying more of a sad emotion. Since emotion is very complex, I have decided to simplify this by plotting the valence (positive and negative) against the arousal (for which I have selected high and low energy). This way, the songs are plotted on the 2D valence-arousal model of Emotion in which each quadrant stands for a different type of basic emotion (angry, happy, sad, peaceful).

**Explanation**  
There seem to be very few peaceful and calm pieces, which corresponds to the characteristics of the band. The only songs that are in this category are from the band with Freddie Mercury as lead singer. After the death of the singer, there seems to be a slight shift towards sad and angry songs. Roger Taylor has more turbulent songs, while Freddie Mercury also has quite a few sad songs. 


### The chromagram of *The Great Pretender* originally from The Platters by Freddie Mercury.

```{r}
Pretender <-
  get_tidy_audio_analysis("4xm2RpQtpFVlPlO1v4BrBr") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Pretender <- Pretender %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "The Great Pretender - Freddie Mercury", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Pretender
```

### And here is an extra, interesting self-similarity matrice of an atypical song by Queen.
```{r}
thirtynine <-
  get_tidy_audio_analysis("6aNP9GlBi3VHPXl7w3Qjr9") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

ThirtyninePlot <- bind_rows(
  thirtynine |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  thirtynine |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16)) +
  labs(x = "", y = "", title = "'39")
ThirtyninePlot
```

