---
title: "It's a kind of magic"
author: "Bente Westermann"
date: 'februari 2023'
output: 
  flexdashboard::flex_dashboard:
    theme:
      bg: "#fffffa"
      fg: "#1c1b1b" 
      primary: "#8fbc8f"
      navbar-bg: "#a9dea9"
      base_font: 
        google: Source Sans Pro
      heading_font:
        google: Sen
---

```{r, setup, echo=FALSE}
# loading packages
library(tidyverse)
library(spotifyr)
library(ggpattern)
library(ggplot2)
library(ggthemes)
library(flexdashboard)
library(plotly)
library(DT)
library(shiny)
library(knitr)
library(grid)
library(gridExtra)
### package by Ashley
library(compmus)

### note to self: 
## 1. knit
## 2. Git, Staged, check
## 3. Commit with message
## 4. push
### ceptrogram
## can tell you how many timbre types there are, whether there is switching between timbres etc.
## beats are useful when the percussion is very clear, otherwise bars are great
```

Chords {.storyboard}
=========================================

### Two chordograms of balads: *Love of My Life* and *Time Waits for No One*. {data-commentary-width=450}

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

major_key <-
  c(5.0, 2.0, 3.5, 2.0, 4.5, 4.0, 2.0, 4.5, 2.0, 3.5, 1.5, 4.0)
minor_key <-
  c(5.0, 2.0, 3.5, 4.5, 2.0, 4.0, 2.0, 4.5, 3.5, 2.0, 1.5, 4.0)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )


time <-
  get_tidy_audio_analysis("0UkCqv8ec7D1W3uGzZa1CP") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

loveofmylife <-
  get_tidy_audio_analysis("2BlNyI35idBaI6BN6WGZeQ") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

turn <-
  get_tidy_audio_analysis("6pDnUKu9z2FKTmwS7TDm3Q") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

rhapsody <-
  get_tidy_audio_analysis("4u7EnebtmKWzUH433cf5Qv") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

TimePlot <- time |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  scale_y_discrete(guide = guide_axis(n.dodge=2)) +
  theme_minimal() +
  labs(title = "Time Waits for No One - Freddie Mercury", x = "", y = "") +
  theme(axis.text.y = element_text(size = 7), 
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        plot.margin = unit(c(5.5, 5.5, 0, 5.5), "pt"))

LovePlot <- loveofmylife |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  scale_y_discrete(guide = guide_axis(n.dodge=2)) +
  theme_minimal() +
  labs(title = "Love of My Life - Queen", x = "Time (s)", y = "") +
  theme(axis.text.y = element_text(size = 7), plot.margin=unit(c(5.5, 5.5, 0, 5.5), "pt"))

TurnPlot <- turn |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Let's Turn it On - Freddie Mercury", x = "Time (s)", y = "")

RhapsodyChordPlot <- rhapsody |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Bohemian Rhapsody - Queen", x = "Time (s)", y = "")

grid.arrange(TimePlot, LovePlot, ncol = 1)
```

***
*(newest plots can be found under General, Keys and Chords)*  
**Explanation**  
These chordograms show the similarity for every bar of a song with certain chords. The more similar the bar is to that specific chord, the darker the color in the graph. When you see bright yellow, it is very unlikely that chords was played in that bar. 

The two songs are, in my opinion, quite similar in style. Both ballads have intricate piano arrangements and are a little more stripped down compared to the rest of the discography. That is why I thought it would be interesting to see how the chordograms compared to one another. 

**Comparison**  
I still need to listen to the songs while having a closer look at the chordograms. But I can already see that there are somewhat more saturated colors in Freddie Mercury's plot. This would mean that the chords are more distinguishable. However, there are four very bright lines in the Queen plot, which could explain why the rest of the figure is less saturated. Another explanation could be that the song by Freddie Mercury is less complicated than the Queen song, I will look at and listen to the songs and the chord progressions to draw a good conclusion.

### Electronic disco song *Let's Turn it On* by Freddie Mercury.

```{r}
TurnPlot
```

***

**Explanation**  
I think it is interesting to see that in this very electronic song, there is not a clear pattern in the chords, whereas in the former two figures there was some sort of dark blue path throughout the plots. All chords that include the 7 have a darker blue line across the entire plot, which is explainable by the way pitches are constructed. 

### Chordogram of *Bohemian Rhapsody*.

```{r}
RhapsodyChordPlot
```

***

**Explanation**  
Since *Bohemian Rhapsody* is such a complex song, I wondered what it would look like in a chordogram. I will also still take a look at it in more detail.

**Conclusion**  


Introduction
=========================================

Column {data-width=600}
-------------------------------------

### "The reason we're successful, darling? My overall charisma, of course." - Freddie Mercury. But was it really?

**Portfolio Computational Musicology**  
Although I am not a '70s kid, Queen is very nostalgic to me. I remember sitting in the backseat of the car, listening to my own CD with all the hits of the famous rock band. I could sing along to every song, even though I did not speak any English. The movie “Bohemian Rhapsody” portrayed (though not completely accurate) a small portion of the lives of the four band members. It reminded me of the solo careers three of the members had, only John Deacon never went solo. The drummer Roger Taylor was the first to release a solo album in 1981. After which Freddie Mercury and Brian May followed. When I heard the song *Time Waits For No One* for the first time not long ago, I thought it was just a Queen song I had not heard before. When I noticed that it was a song exclusively by Freddie Mercury, I wondered how it differed from songs produced by the band. Freddie Mercury has had less success as a solo artist than together with his three band members, what could have been the reason for this? 

Therefore, I will be comparing solo songs by Freddie Mercury with the songs from the band Queen. In addition to that, I am curious whether his solo career had any influence on the songs that were produced afterwards together with the band members. I will also take a look at the solo songs by Roger Taylor as he was the first to go solo. At the end of my page, there will also be a small comparison between Queen with Freddie Mercury as lead singer, and after the singer's death.

**Expectations** 
The band Queen is known for their grandiosity and experimentation. They were not afraid to deviate from the norm and tried to mix all kinds of genres together in their songs including rock, pop, metal, opera and electronic music. It is therefore very difficult to pinpoint the characteristics of Queen and to select typical songs. However, many songs seem to be theatrical or dramatic featuring intricate arrangements and harmonies, the lyrics have a message and explore complex themes, and the songs are pushing boundaries of rock music. These qualities can be clearly heard in what may be the most famous Queen song *Bohemian Rhapsody*, which includes the complex harmonies and shifts between different genres such as rock, pop and opera. It beautifully displays Freddie Mercury's impressive range and theatrical feeling.

Needless to say, these iconic vocals can also be heard in the solo songs released by Freddie Mercury. He did seem to enjoy Disco music more than the other band members. Disco music is known for its great danceability. Therefore, the genre of disco music is characterized by a strong and steady beat, often resulting in a 4/4 time signature. To add more interest and to give songs more energy, guitar bass lines and drum hi-hats frequently contained syncopated rhythms. Disco music also uses a lot of different instruments and has repetitive but funky and soulful vocals. The classic disco beat can be heard for example in the songs *Love Kills*, *Living on My Own* and *I Was Born to Love You* on his album Mr. Bad Guy. *Your Kind of Lover* from the same album also includes a typical disco bassline.  
In addition to disco, opera is also a common genre in the songs released by Freddie Mercury. Together with opera singer Montserrat Caballé he has brought out an album, from which the song with the same name as the album, *Barcelona*, is most famous. These partially operatic songs will differ from the Queen songs in genre and pitch.  
Since Freddie Mercury was the chief songwriter of the band Queen, I expect lots of similarities in the songwriting such as catchy hooks and melodies, but I also anticipate Queen having more complex and grandiose songs while Freddie Mercury will probably be more stripped down and focused on vocals, piano and danceability.

**Representation**  
I have tried to make as complete a list as possible of the solo songs by Freddie Mercury. Nevertheless, there are a few non-original songs from the two albums he has released (*Mr. Bad Guy* and *Barcelona*) because those songs only are on Spotify as special editions (listed in *Song list 1.1*) or new orchestrated editions (listed in *Song list 1.2*). I have decided to include the songs to have a bigger set of data since there already are much fewer solo songs by Freddie Mercury (N = 25) in comparison to songs by the band Queen (N = 159).
Regarding the band, I have made a playlist with all albums going from 1973 till 1995. Each Queen song is remastered in 2011, while Spotify only states that *In My Defence* by Freddie Mercury is remixed in 2000. In *Song list 2.1* there are a few songs that stand out in the discography of Queen. In addition to these songs, the tracks from before *Killer Queen* sound less clear than all songs that come after due to reverb on the vocals and guitar.

In addition, I have created a small playlist (N = 13) with the songs that were released by the band after the death of Freddie Mercury so I can investigate how Freddie Mercury's death affected the band. This playlist contains one track called *Yeah* which is precisely what the title suggests: namely 4 seconds of Freddie Mercury singing "Yeah".

The playlist of songs by Roger Taylor consists of 44 songs from four different albums. 

Song list 1.1: Special Editions

  - Foolin' Around
  - Your Kind of Lover
  - Mr. Bad Guy
  - Man Made Paradise
  - There Must Be More to Life Than This

Song list 1.2: New Orchestrated Editions

  - La Japonaise
  - Ensueño
  - Guide Me Home

Song list 2.1

  - Ming's Theme
  - The Ring
  - The Hitman
  - Ogre Battle
  - '39

Column {data-width=200}
-------------------------------------

### Queen playlist
<iframe src="https://open.spotify.com/embed/playlist/3FOkKYT1wXXJH8L2lcSP5t?utm_source=generator" width="100%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

Column {data-width=200}
-------------------------------------

### Freddie Mercury playlist
<iframe src="https://open.spotify.com/embed/playlist/7IXPxFygCKwBEklOVopUu8?utm_source=generator" width="100%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>


General {.storyboard}
=========================================

### The most popular songs are danceable, happy and have sung lyrics. {data-commentary-width=600}

```{r}
options(scipen=999)
options(digits = 3)

# my playlists
logo <- list.files("/Users/gebruiker/Documents/Studies/CLC/Jaar 2 2022-2023/Computational Musicology/Portfolio-Comp-Musicology", pattern = ".png", full.names = TRUE)
freddie <- get_playlist_audio_features('', '7IXPxFygCKwBEklOVopUu8') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
freddie$track.album.release_date = c(1987, 1992, 1984, 1988, 1985, 1985, 1988, 1989, 1985, 1985, 1988, 1988, 1985, 2019, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1988, 1988, 1988)

queen <- get_playlist_audio_features('', '3FOkKYT1wXXJH8L2lcSP5t') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

after <- get_playlist_audio_features('', '2Oou3UXp7fHZsHHxQ9jQnC') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
roger <- get_playlist_audio_features('', '7eYTx9OtJPrKBst4bWr3PD') %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

Solo <- freddie %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Band <- queen %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)
Roger <- roger %>% summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    mean_mode = mean(mode),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_key = mean(key),
    mean_valence = mean(valence),
    mean_tempo = mean(tempo),
    mean_pop = mean(track.popularity),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    sd_mode = sd(mode),
    sd_danceability = sd(danceability),
    sd_energy = sd(energy),
    sd_key = sd(key),
    sd_valence = sd(valence),
    sd_tempo = sd(tempo),
    sd_pop = sd(track.popularity),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    median_mode = median(mode),
    median_danceability = median(danceability),
    median_energy = median(energy),
    median_key = median(key),
    median_valence = median(valence),
    median_tempo = median(tempo),
    median_pop = median(track.popularity),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness),
    mad_mode = mad(mode),
    mad_danceability = mad(danceability),
    mad_energy = mad(energy),
    mad_key = mad(key),
    mad_valence = mad(valence),
    mad_tempo = mad(tempo),
    mad_pop = mad(track.popularity)
)

Comparison1 <-
  bind_rows(
    queen %>% mutate(category = "Band (Queen)"),
    freddie %>% mutate(category = "Freddie Mercury"),
    roger %>% mutate(category = "Roger Taylor")
  )
Comparison1 <- Comparison1 %>% rename(popularity = track.popularity, track = track.name)

Comparison2 <-
  bind_rows(
    queen %>% mutate(category = "Band (Queen)"),
    freddie %>% mutate(category = "Freddie Mercury"),
    after %>% mutate(category = "After the death"),
    roger %>% mutate(category = "Roger Taylor")
  )
Comparison2 <- Comparison2 %>% rename(popularity = track.popularity, track = track.name)

SumComparison <- 
    bind_rows(
    Band %>% mutate(category = "Band"),
    Solo %>% mutate(category = "Solo"),
    Roger %>% mutate(category = "Roger")
  )

# creating a color blind friendly color palette
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", 
"#CC79A7","#000000")
gray <- gray.colors(5, start = 0, end = 1, gamma = 2.2, rev = TRUE)
safe_palette <- c("#007B71", "#B6231B", "#977400", "#a9dea9", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")
palette2 <- c("#B6231B", "#977400", "#a9dea9", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")
                           
plot1 <- ggplot(Comparison2, aes(x = tempo, y = danceability, fill = category, color = as.factor(time_signature))) +
  geom_point(shape = 21, size = 3.5, stroke = 1) +
  labs(y = 'Danceability', x = 'Tempo', fill = 'Artist', color = 'Beats per Measure') +
  scale_fill_manual(values=alpha((safe_palette), 0.6)) +
  scale_color_manual(values=gray) +
  scale_y_continuous(limits = c(0,1)) +
  geom_label(label="Yeah - Queen", x=25, y=0.005, label.padding = unit(0.20, "lines"), color = "black", fill = "white") +
  theme_minimal() +
  theme(legend.position = c(0.15, 0.6), axis.text.y = element_blank())

plot3 <- ggplot(Comparison1, aes(x = danceability, y = popularity, color = valence, size = instrumentalness, text = paste("Track: ", track, "<br>",
                          "Artist: ", category, "<br>",
                          "Popularity: ", popularity, "<br>",
                          "Danceability: ", danceability, "<br>",
                          "Valence: ", valence, "<br>",
                          "Instrumentalness: ", instrumentalness))) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c() +
  labs(title = "The Danceability of Songs by Queen and Members of the Band", x = "Danceability", y = "Track Popularity", size = "Instrumentalness", color = "Valence") +
  facet_wrap(~category) +
  theme_bw() +
  theme(strip.background = element_rect(fill="#a9dea9"))

DanceabilityPlotly <- ggplotly(plot3, tooltip = "text")
DanceabilityPlotly

Dust <- Comparison1 %>% filter(track == "Another One Bites The Dust - Remastered 2011")
PopRoger <- Comparison1 %>% filter(track == "Man On Fire")
Forever <- Comparison1 %>% filter(track == "Who Wants To Live Forever - Remastered 2011")
```

***
**Popularity**  
The first thing that stands out is that Roger Taylor is far less popular than Queen and Freddie Mercury. The most popular song by Roger Taylor is *Man On Fire* and has a popularity of `r PopRoger$popularity`. The three most popular songs by Queen are *Bohemian Rhapsody*, *Another One Bites The Dust* and *Don't Stop Me Now* which all have a popularity of `r Dust$popularity`. Danceability and popularity seem to be correlated in Queen and Roger Taylor songs; the more danceable songs are also more popular. Within the songs of Freddie Mercury this pattern can not be found.

**Instrumentalness**  
As for instrumentalness, Spotify was not too precise, since the song with the highest score was *Seven Seas of Rhye* which does have sung lyrics. The other songs at the top, though, are *The Ring*, *Vultan's Theme*, *God Save the Queen* and *Escape From The Swamp* which are all completely instrumental. In the top 10 most instrumental songs, seven are from the album *Flash Gordon* which is an album made for the homonymous science fiction movie. It seems that the more instrumental a song is, the less popular it is on Spotify. There is one exception to this rule: *Another One Bites the Dust* is one of the most popular songs by Queen but has an instrumentalness of `r Dust$instrumentalness`. 

**Valence**  
The happiest song according to Spotify is *Misfire* by Queen. *Rain Must Fall* is following with only `r 0.001` points in difference. Both songs have a danceability score above the mean of `r Band$mean_danceability`. This seems to be fitting the norm; the happier a song is, the higher it rates on danceability. The lowest valence scores are from the songs *Ming's Theme* and *The Ring (Hypnotic Seduction Of Dale)* which also both are very instrumental. The next song with the lowest valence is a more typical song by the band, namely *Who Wants To Live Forever*. This song also has the lowest danceability score (`r Forever$danceability`).

### Durational features {data-commentary-width=500}

```{r}
freddie <-
  get_playlist_audio_features(
    "",
    "7IXPxFygCKwBEklOVopUu8"
  ) |> add_audio_analysis() %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )
freddie$track.album.release_date = c(1987, 1992, 1984, 1988, 1985, 1985, 1988, 1989, 1985, 1985, 1988, 1988, 1985, 2019, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1988, 1988, 1988)


queen <-
  get_playlist_audio_features(
    "",
    "3FOkKYT1wXXJH8L2lcSP5t"
  ) |> add_audio_analysis() %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

roger <-
  get_playlist_audio_features(
    "",
    "7eYTx9OtJPrKBst4bWr3PD"
  ) |> add_audio_analysis() %>%
  mutate(
    track.album.release_date =
      map_int(
        track.album.release_date,
        \(x) as.integer(str_sub(x, start = 1, end = 4))
      )
  )

comp <-
  freddie |>
  mutate(category = "Freddie Mercury") |>
  bind_rows(queen |> mutate(category = "Band (Queen)"), roger |> mutate(category = "Roger Taylor"))

comp <- comp %>% rename(track = track.name)

Tempo <- comp |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections)
TempoPlot <- ggplot(Tempo,
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = category,
      alpha = factor(loudness),
      size = time_signature, 
      text = paste("Track: ", track, "<br>",
                          "Artist: ", category, "<br>",
                          "Release date: ", track.album.release_date, "<br>",
                          "Tempo: ", tempo, "<br>",
                          "Loudness: ", loudness, "<br>",
                          "Time signature: ", time_signature)
    )
  ) +
  geom_point() +
  geom_rug() +
  scale_color_manual(values = palette2) +
  scale_size_continuous(range = c(1, 6), breaks = seq(1, 6, by = 0.05), trans = "reverse") +
  scale_alpha_discrete(guide = "none") +
  annotate("text", x = 70, y = 4.8, label = "Queen") +
  annotate("text", x = 77, y = 4.3, label = "Freddie Mercury") +
  annotate("text", x = 74.2, y = 3.8, label = "Roger Taylor") +
  annotate("rect", xmin = 62 ,xmax = 64, ymin = 4.75 ,ymax = 4.89, fill = "#B6231B") +
  annotate("rect", xmin = 62 ,xmax = 64, ymin = 4.25 ,ymax = 4.39, fill = "#977400") +
  annotate("rect", xmin = 62 ,xmax = 64, ymin = 3.75 ,ymax = 3.89, fill = "#a9dea9") +
  theme_bw() +
  theme(legend.position='none') +
  ylim(0, 5) +
  labs(
    title = "Durational Features of Songs by Queen and Members of the Band",
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Artist",
    size = "Time Signature (beats)",
    alpha = "Volume (dBFS)"
  )

TempoPlotly <- ggplotly(TempoPlot, tooltip = "text")
TempoPlotly
#TempoPlotly %>%
#  style(hoverinfo = "text",
 #       hovertext = paste("Track: ", Tempo$track, "<br>",
  #                        "Artist: ", Tempo$category, "<br>",
   #                       "Tempo: ", Tempo$tempo, "<br>",
    #                      "Loudness: ", Tempo$loudness, "<br>",
     #                     "Beats per measure: ", Tempo$time_signature))
```
***

**Explanation**  
Here you can see the mean tempo, the deviation of each song from that mean tempo, the loudness in opacity
and the beats per measure signature in the size of the point. The biggest deviation can be found in the oldest Queen songs, which can be seen in the labels if you hover over the points, or in the songs with either a relatively slow or fast tempo. Older Queen songs were recorded using analog equipment, which may have had limitations in terms of capturing precise tempos. This could have led to more variation in tempo in their older recordings. It might also be, that the earlier songs were even more experimental than later songs and incorporated more styles that suited more tempo changes.

**Time signature**  
Spotify API had a hard time with the songs *Execution of Flash* as well as *Ming's Theme* by Queen because there is no consistent time signature. Both songs are from the album *Flash Gordon* and as you will see in the chromagram section, this is not a typical album. The song *Dear Friends* by Queen also stands out; it states that there is only one beat per measure but when I listened to the song, I felt like the time signature was 4/4. I do have to say that the arrangement with the piano emphasized all beats very equally, that might be an explanation for the weird time signature.

### Timbre features
```{r}
freddie <-
  get_playlist_audio_features(
    "",
    "7IXPxFygCKwBEklOVopUu8"
  ) |>
  add_audio_analysis()

queen <-
  get_playlist_audio_features(
    "",
    "3FOkKYT1wXXJH8L2lcSP5t"
  ) |> slice(1:25) |>
  add_audio_analysis()

queen2 <-
  get_playlist_audio_features(
    "",
    "3FOkKYT1wXXJH8L2lcSP5t"
  ) |> slice(135:159) |>
  add_audio_analysis()

comp <-
  freddie |>
  mutate(category = "Freddie Mercury (1985)") |>
  bind_rows(queen |> mutate(category = "Queen (1973)"), queen2 |> mutate(category = "Queen (1990)"))

comp <- comp %>% rename(track = track.name)

comp |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(category, timbre) |>
  compmus_gather_timbre() |>
  ggplot(aes(x = basis, y = value, fill = category)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Artist")
```

***

**Explanation**  
For every 12 levels of timbre used by Spotify API I have plotted the range for the songs by Freddie Mercury, the oldest songs by Queen (released around the year 1973) and the newest songs with Freddie Mercury still as lead singer (released around the year 1990). I am not sure I will keep this plot in, because I cannot see very interesting differences between the three categories.

### After Freddie Mercury's death, the sadness could be felt in the songs by Queen. {data-commentary-width=500}

```{r}
plot2 <- ggplot(data = Comparison2) +
  geom_vline(xintercept = 0.5) +
  geom_hline(yintercept = 0.5) +
  geom_point(aes(x = valence, y = energy, color = category, text = paste(
    "Track: ", track, "<br>",
    "Artist: ", category, "<br>",
    "Energy: ", energy, "<br>",
    "Valence: ", valence, "<br>",
    "Key: ", key_mode)), size = 3, alpha = 0.7) +
  labs(y = 'Energy', x = 'Valence', color = 'Artist') +
  scale_x_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(-0.1, 1.1)) +
  annotate('text', 0.25 / 4, 1.05, label = "Turbulent/Angry", fontface = "bold") +
  annotate('text', 1-(0.25/4), 1.05, label = "Happy/Joyful", fontface = "bold") +
  annotate('text', 1-(0.25/4), -0.05, label = "Chill/Peaceful", fontface = "bold") +
  annotate('text', 0.25 / 4, -0.05, label = "Sad/Depressing", fontface = "bold") +
  #annotate(geom = "label", 0.75, 0.05, label = "Dear Friends - Queen", fill = "#a9dea9") +
  scale_color_manual(values=safe_palette) +
  theme_bw() +
  theme(legend.position = c(0.76, 0.23), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank())

EmotionPlotly <- ggplotly(plot2, tooltip = "text")
EmotionPlotly 
```
***
**Emotion Model**  
I was curious about the emotion every song portrays. Freddie Mercury has once said that he is very emotional. "I think all my songs are under the label emotion. The more I open up, the more I get hurt, so basically what happens is I'm just riddled with scars." After Freddie Mercury's death, I could see songs portraying more of a sad emotion. Since emotion is very complex, I have decided to simplify this by plotting the valence (positive and negative) against the arousal (for which I have selected high and low energy). This way, the songs are plotted on the 2D valence-arousal model of Emotion in which each quadrant stands for a different type of basic emotion (angry, happy, sad, peaceful).

**Explanation**  
There seem to be very few peaceful and calm pieces, which corresponds to the characteristics of the band. The only songs that are in this category are from the band with Freddie Mercury as lead singer. After the death of the singer, there seems to be a slight shift towards sad and angry songs. Roger Taylor has more turbulent songs, while Freddie Mercury also has quite a few sad songs. 

Chromagram {.storyboard}
=========================================

### Chromagram of a very atypical song by Queen (*Ming's Theme*) shows simplicity. {data-commentary-width=600}

```{r}
Born <-
  get_tidy_audio_analysis("14FAtIQZsRYeu8zVI33l7f") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Born <- Born %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "I Was Born To Love You - Freddie Mercury", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Stop <-
  get_tidy_audio_analysis("5T8EDUDqKcs6OSOwEsfqG7") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Stop <- Stop %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Don't Stop Me Now - Queen", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Ming <-
  get_tidy_audio_analysis("1kcsmxEal6ZbVF2N0FAVeT") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Ming <- Ming %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Ming's Theme - Queen", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Ming
```

***
**Chromagram**  
I have created three chromagrams to show the difference between a simple, atypical Queen song and complex Queen and Freddie Mercury songs that are very representative for their style. A chromagram shows the the energy for every pitch class during a song. 

**Explanation**  
A very atypical song by Queen is *Ming's Theme (In The Court Of Ming The Merciless)*. This song is part of the album that was made specifically for the movie *Flash Gordon*. It starts with low pitched electronic sounds and later on includes people talking. In the chromagram you can see how the song starts on a note that is somewhere around F sharp. There are two descending melodies leading to a note between C and C sharp. After this, the talking starts with the same note softly sounding in the background. Around two minutes into the song there is another descending melody. The melodic part ends with a perfect fifth (D and A). The song concludes with a dialogue from the movie.

**Comparison**  
On the next page the chromagram of *Don't Stop Me Now* and *I Was Born To Love You* can be seen. You will see that this clear distinction of notes and sections can not be made in a more typical Queen song, also in a Freddie Mercury song this is much harder. Even the key can not be easily identified.

<iframe src="https://open.spotify.com/embed/playlist/1jqc3j3k6VXu5RykFegI5x?utm_source=generator" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

### Typical Queen and Freddie Mercury songs, however, are far from simple. {data-commentary-width=750}

```{r, fig.height=4, fig.width=5}
Stop
```

*** 
**Queen**  
For Queen, I think *Don't Stop Me Now* is a very representative song. It is very energetic, melodic and euphoric. It is also one of the most popular songs according to Spotify, along with *Bohemian Rhapsody* and *Another One Bites The Dust*.


```{r, fig.height=4.5, fig.width=5.5, out.extra='style="float:right; padding:10px"'}
Born
```
**Freddie Mercury**  
For Freddie Mercury as a solo artist I have chosen the song *I Was Born To Love You*, as it is the most popular song according to Spotify. I also think it is quite representative for Freddie Mercury, as it has a disco feeling and is very lively. The song *The Great Pretender* is equally popular, but is a song originally by The Platters and written by Buck Ram (their manager).  
I have also plotted *The Great Pretender* as a chromagram (can be seen on the next tab) and interestingly this was much more readable than original Queen and Freddie Mercury songs, just like *Ming's Theme*.

**Comparison**  
The chromagram shows that *Don't Stop Me Now* contains lots of pitch classes and it has thus very complex harmonies, while *I Was Born To Love You* seems to have a bit more of distinct pitch classes with high energy. The highest energy can be found in the class of G sharp, this is also the key of the song. For *Don't Stop Me Now* it is harder to see the key, though the pitch class G seems to have a slightly bigger magnitude than the other pitch classes, this song is in the key of F major.

### The chromagram of *The Great Pretender* originally from The Platters by Freddie Mercury.

```{r}
Pretender <-
  get_tidy_audio_analysis("4xm2RpQtpFVlPlO1v4BrBr") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Pretender <- Pretender %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "The Great Pretender - Freddie Mercury", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16)) +
  scale_fill_viridis_c()

Pretender
```

***
**Explanation**  
As said before, this song is originally by the Platters and covered by Freddie Mercury. In the chromagram you can clearly see that the song starts in G major from the frequent bright yellow stripes, meaning that there is most energy in the pitch class G, and modulates at around 2 minutes to G sharp major. The pitch classes C and D are also used repeatedly in the first 120 seconds. Freddie Mercury loved the song *The Great Pretender* for its meaning as he felt like he played different roles on stage; he went through different moods, wore different costumes and became someone else with each costume. 

**Conclusion**  
Both Queen and Freddie Mercury songs are known for the intricate harmonies, which the chromagrams seem to confirm. Though the solo work might be slightly less complex and sometimes a little more stripped down to focus on the rhythm and energy.

Self-similarity {.storyboard}
=========================================

### Queen songs tend to deviate from the clear structure that is frequent in popular music, unless Roger Taylor writes the song. {data-commentary-width=450}
```{r, fig.width=10, fig.height=9}
### different song
rhapsody <-
  get_tidy_audio_analysis("4u7EnebtmKWzUH433cf5Qv") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

car <-
  get_tidy_audio_analysis("253ewjP2R1LjKqA70mGdnk") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

CarPlot <- bind_rows(
  car |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  car |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  #theme(plot.title = element_text(size = 14)) +
  labs(x = "", y = "", title = "I'm in Love With My Car - Queen")

RhapsodyPlot <- bind_rows(
  rhapsody |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  rhapsody |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "", y = "", title = "Bohemian Rhapsody - Queen") +
  theme_minimal()

grid.arrange(CarPlot, RhapsodyPlot, ncol = 1)
```

***
**Self-similarity matrices**  
These are four self-similarity matrices, with the axes in seconds, that show the pitch- and timbre-based self-similarity for every bar. At the top you can see a song that is written and sung by the drummer Roger Taylor but released as a Queen song. On the bottom the song Bohemian Rhapsody, written by Freddie Mercury, can be seen. The song written by Roger Taylor seems to have much more of a uniform structure than *Bohemian Rhapsody*, which can be derived from the checkerboard pattern.

**Explanation**  
The song *I'm in Love With My Car* starts with a guitar riff accompanied with repeating D chords on the piano, followed by verse 1 that makes use of four different chords that are repeated after one another. After 60 seconds, the chorus starts in which Freddie Mercury sings backing vocals. There is a slight change in color in the timbre-based matrix, but I expected to see a greater change. The chords stay the same, thus the transition from verse 1 to the chorus is not really visible in the chroma-based matrix. At around 1:20 minutes (80 seconds) into the song, the chords change. This can be clearly seen in the chroma-based matrix, but is less apparent in the timbre-based one. That the guitar riff and accompanying piano chords recur after this short change becomes evident from both matrices. Then there is another verse that follows the same structure as verse 1, but does include guitar riffs in the background. The two matrices pick up the repetition in the second chorus very well, resulting in diagonal lines at around 2:10 minutes (130 seconds). They play once again some guitar riffs accompanied by the repeating chords on the piano that slowly fade out with car noises sounding on top of that. The song ends with some chords on the guitar that sound very different (which explains the bright yellow line at the end of the timbre-based matrix) and car noises.

### Songs by Freddie Mercury as a solo artist also have a clearer structure. {data-commentary-width=450}

```{r}
livingown <-
  get_tidy_audio_analysis("5EyrSES4OYcUxpZdhO1MQj") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

LivingPlot <- bind_rows(
  livingown |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  livingown |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 11)) +
  labs(x = "", y = "", title = "Living on My Own - Freddie Mercury")
LivingPlot
```

***
**Comparison**  
As I explained in the introduction, Queen is known for their experimentation and deviation from the norm. On the previous tab this could be seen in the matrices of the song *Bohemian Rhapsody*, since there was some structure visible but not how you would expect a typical rock or pop song to be arranged. The song that was written by Roger Taylor had much more of a common and clear structure, as is the case with a very typical song by Freddie Mercury as a solo artist. On this page you can see the timbre- and pitch-based self-similarity matrices of the song *Living on My Own*. The song is in my view a classic Freddie Mercury song, that features Freddie's distinctive voice, flamboyant personality, and his love for dance music. The song's upbeat tempo, catchy melody, and electronic production are all hallmarks of Freddie's solo work. Additionally, the song features a memorable chorus, which is typical of Freddie's ability to write catchy songs.

**Freddie Mercury**  
The chroma-based matrix illustrates nicely through the diagonal lines that there are lots of repetitions in the song. The songs starts with A minor chords and when the next chord is introduced, there is a yellow line in the chroma-based matrix (at around 25 seconds). At around 50 seconds, the chorus is introduced. In the same matrix it becomes clear that the chorus repeats after another 35 seconds (approximately 85 seconds into the song). Next there is a break with some scatting and improvisation on the piano, followed by halve of the chorus. The beat that is used throughout the whole song accompanied by piano chords and some occasional scatting are utilized as an outro. Though the true ending is the artificial repetition of one ad lib by Freddie Mercury.

### And here is an extra, interesting self-similarity matrice of an atypical song by Queen written by Brian May.
```{r}
thirtynine <-
  get_tidy_audio_analysis("6aNP9GlBi3VHPXl7w3Qjr9") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

ThirtyninePlot <- bind_rows(
  thirtynine |> 
    compmus_self_similarity(pitches, "euclidean") |> 
    mutate(d = d / max(d), type = "Chroma"),
  thirtynine |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16)) +
  labs(x = "", y = "", title = "'39 - Queen")
ThirtyninePlot
```

***

**Explanation**  
Coincidentally, this song is also not written by Freddie Mercury but by Brian May. I found this song very interesting, because of it deviates greatly from the normal style of Queen. It is a blend of folk and progressive rock with acoustic instruments, vocal harmonies, and storytelling lyrics. At around 35 seconds into the song the beat comes in. This interestingly leads to a change in the chroma matrix but a much less clear change in the timbre matrix. After one and a half minutes the bridge introduces new chords. I find it very strange that this is so clear in the timbre matrix but not in the chroma matrix. It almost feels as though the matrices are switched around. Specially because the bright yellow cross in the chroma matrix is exactly where the electric guitar makes an appearance.

<iframe src="https://open.spotify.com/embed/playlist/2POKY9r3mluVe4rkDzS0AA?utm_source=generator" width="100%" height="85" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

Keys {.storyboard}
=========================================

### After the death of Freddie Mercury the band released most songs in G major, just like Freddie Mercury in his solo career.
```{r}
KeysRatio <- Comparison2 %>%
  group_by(category, key_mode) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ungroup()
KeysPlotRatio <- ggplot(KeysRatio, aes(key_mode, freq, fill = category, text = paste("Artist: ", category, "<br>",
                          "Key: ", key_mode, "<br>",
                          "Percentage: ", freq, "<br>",
                          "Actual frequency:", n))) +
  geom_col(position = position_dodge(preserve = "single", width = 0.8), alpha = 0.8, width = 1) +
  scale_fill_manual(values=safe_palette) +
  labs(title = "Distribution of Keys", x = "Key", y = "Frequency in Probability", fill = "Artist", fontface = "bold", angle = 90, size = 2) +
  theme_bw() +
  theme(legend.position = c(0.76, 0.23), axis.text.x = element_text(angle = 90), axis.ticks.x = element_blank(), axis.ticks.y = element_blank())

roger <-
  get_playlist_audio_features(
    "",
    "7eYTx9OtJPrKBst4bWr3PD"
  ) |>
  add_audio_analysis()

after <-
  get_playlist_audio_features(
    "",
    "2Oou3UXp7fHZsHHxQ9jQnC"
  ) |>
  add_audio_analysis()

comp2 <-
  after |> mutate(category = "After") |>
  bind_rows(freddie |> mutate(category = "Freddie"), roger |> mutate(category = "Roger"), queen |> mutate(category = "Band (Queen)"))

KeysPlot <- ggplot(KeysRatio, aes(key_mode, n, fill = category, text = paste("Artist: ", category, "<br>",
                          "Key: ", key_mode, "<br>",
                          "Actual frequency:", n))) +
  geom_col(width = 1) +
  scale_fill_manual(values=safe_palette) +
  labs(title = "Distribution of Keys", x = "Key", y = "Frequency", fill = "Artist", fontface = "bold", angle = 90, size = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90), axis.ticks.x = element_blank(), axis.ticks.y = element_blank(), strip.background = element_rect(fill="#a9dea9")) +
  facet_wrap(~category)

KeysPlotlyRatio <- ggplotly(KeysPlotRatio, tooltip = "text")
KeysPlotlyRatio

NTracks <- Comparison2 %>% group_by(category) %>% summarise(Tracks = n()) %>% rename(Artist = category)
```
***

**Explanation**  
Here you can see the distribution of keys for every artist. In the next tab you can see the actual amounts (which can also be seen in if you hover over the points), but as there is such a difference in playlist length (see the table), I have chosen to first plot the ratio.

Interestingly, the key G major was very loved by Freddie Mercury, Roger Taylor and the band after the death of the lead singer. Nevertheless, before that, the band used the key D major the most frequent.

**Numbers of Tracks per Artist**  
`r NTracks %>% knitr::kable()`

### Distribution of keys for every song in actual amounts.
```{r}
ggplotly(KeysPlot, tooltip = "text")
```
***
**Numbers of Tracks per Artist**  
`r NTracks %>% knitr::kable()`

Conclusion
=========================================

Column {data-width=400}
-------------------------------------

### “I won’t be a rock star. I’ll be a legend.” - Freddie Mercury. A legend he is and here is why.

`r knitr::include_graphics(logo)`

Column {data-width=600}
-------------------------------------

### Conclusion

From the plots that I made we can already see that there is a lot of overlap between the songs by Freddie Mercury and the band Queen. Nevertheless, Queen has much more popular songs than Freddie Mercury as a solo artist. The popular songs are mostly happy, danceable and include lyrics. Queen has made use of complex harmonies and the songs are high in energy. Freddie Mercury's songs are also complex but not all popular ones are also danceable. 

I will look more into durational features in the next couple of weeks as rhythm is a big part of what is  characteristic to disco music. Disco music was beloved by Freddie Mercury, but not by the other three band members. Thus, I am curious to see whether there is a difference that could help explain the distinction between the band Queen and Freddie Mercury.